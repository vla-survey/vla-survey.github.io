カテゴリ,略称,タイトル,学会,著者所属,Paper URL,Website URL,タスク,Domain,ロボット,Training,Evaluation,Modality,Dataset,Backbone,Action Generation,概要
Review,,A Survey on Vision-Language-Action Models for Embodied AI,,China,https://arxiv.org/abs/2405.14093,,,,,,,,,,,"非常に幅広いが, LLMのplanningへの利用なども含み, 近年のVLAに対するsurveyとはなっていない"
Review,,"Vision-Language-Action Models: Concepts, Progress, Applications and Challenges",,"Cornell University, HKUST",https://arxiv.org/abs/2505.04769,,,,,,,,,,,"近年のVLAを良くまとめているが, 体系的にアーキテクチャがまとまっているわけではない.  また, affordanceによるやつとかは無視している"
Review,,A Survey on Vision-Language-Action Models: An Action Tokenization Perspective,,"Peking University, PsiBot",https://arxiv.org/abs/2507.01925,,,,,,,,,,,
"3D, End-to-End, Policy",Mamba Policy,Mamba Policy: Towards Efficient 3D Diffusion Policy with Hybrid Selective State Models,IROS,"Beijing Innovation Center of Humanoid Robotics, HKUST, Zhejiang University",https://arxiv.org/abs/2409.07163,https://andycao1125.github.io/mamba_policy/,Manipulation,Tabletop,Franka Emika Panda,Supervised,"Adroit, DexArt, MetaWorld","PointCloud, Proprioception",,Mamba,Diffusion,Mambaアーキテクチャを用いたPolicy
"Planning, Policy",ViLa,Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning,,"Shanghai Artificial Intelligence Laboratory, Tsinghua University",https://arxiv.org/abs/2311.17842,https://robot-vila.github.io/,Manipulation,Tabletop,Franka Emika Panda,,RAVENS,"Language, Vision",,GPT,Primitive,ChatGPT-4Vを使ってsubtaskに分解+実行
Special,VLATest,VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation,,,https://arxiv.org/abs/2409.12894,,,,,,,,,,,7つのVLAのperformanceを比較
Special,RevLA,ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models,,,https://arxiv.org/abs/2409.15250,,,,,,,,,,,vision encoderの破壊的忘却を防ぐには？
Special,Adversarial,Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics,,,https://arxiv.org/abs/2411.13587,,,,,,,,,,,VLAのadversarial attackについて
Special,,Grounding Multimodal Large Language Models in Actions,,,https://arxiv.org/abs/2406.07904,,,,,,,,,,,どのaction space representationが一番良いのか？
Special,,Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture,,,https://arxiv.org/abs/2502.04558,,,,,,,,,,,
Special,,"Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks",,,https://arxiv.org/abs/2411.05821,,,,,,,,,,,
Special,,Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models,,,https://arxiv.org/abs/2409.13174,,,,,,,,,,,
Dataset,Epic-Kitchens,Scaling Egocentric Vision: The EPIC-KITCHENS Dataset,ECCV,University of Bristol,https://arxiv.org/abs/1804.02748,https://epic-kitchens.github.io/2025,,,Human,,,,,,,EPIC-KITCHENSは、55時間の自然なキッチン活動をマルチアノテーションして作成されたEPIC-KITCHENSは、オブジェクト・行動・未来予測を網羅する最大規模のegocentricデータセットであり、既存の手法が20%以下の性能にとどまるほど難易度が高い。このベンチマークは、実使用時点でのビジョン研究の新たな基準点と課題を提示します。
Dataset,Robosuite,robosuite: A Modular Simulation Framework and Benchmark for Robot Learning,,University of Texas at Austin,https://arxiv.org/abs/2009.12293,https://robosuite.ai/,Manipulation,,Simulation,,,,,,,"robosuite v1.5は、MuJoCoベースのモジュラーシミュレーションと9つのベンチマークタスクにより、再現可能なロボット学習研究をサポートします。
さまざまなロボット・センサー・コントローラの組み合わせと手続き型環境APIにより、新しいタスクの定義とsim-to-real研究を迅速に行うことができます。"
VLM for Robotics,CLIP,Learning Transferable Visual Models From Natural Language Supervision,,,https://arxiv.org/abs/2103.00020,,,,,,,,,,,
"Affordance, End-to-End",CLIPort,CLIPort: What and Where Pathways for Robotic Manipulation,CoRL,"NVIDIA, University of Washington",https://arxiv.org/abs/2109.12098,https://cliport.github.io/,Manipulation,Tabletop,Franka Emika Panda,Supervised,RAVENS,"Depth, Language, Vision",,CLIP,Heatmap,Transporter Network + CLIP
Dataset,Ego4D,"Ego4D: Around the World in 3,000 Hours of Egocentric Video",CVPR,"Carnegie Mellon University, Georgia Tech, MIT, Meta, National University of Singapore, The University of Tokyo, UC Berkeley, University of Bristol, University of Minnesota, University of Pennsylvania, University of Texas at Austin",https://arxiv.org/abs/2110.07058,https://ego4d-data.org/,,,Human,,,,,,,"Ego4Dは、3,670時間-931人規模の世界最大の1人称マルチモーダル映像データセットと5つのベンチマーク（過去・現在・未来）を提供し、ロボット・AR・視覚知能研究の標準を新たに定義する。実世界の日常を忠実に再現し、「人の視点認識」のスケール・多様性・現実性を一段と高めた。"
Evaluation,CALVIN,CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks,RAL,"University of Freiburg, University of Technology Nuremberg",https://arxiv.org/abs/2112.03227,http://calvin.cs.uni-freiburg.de/,Manipulation,,Simulation,,,,,,,CALVINは、ロボットが自然言語指示による長時間の操作タスクを学習・評価できるように、4つの環境、34の課題、マルチモーダルセンサを備えたベンチマーク・データセットです。MCILベースラインは、単一の課題の一部のみを解決し、言語・行動連結・長期計画・ドメイン遷移研究の広い余地を明らかにしています。
VLM for Robotics,MVP,Masked Visual Pre-training for Motor Control,CoRL,UC Berkeley,https://arxiv.org/abs/2203.06173,https://tetexiao.com/projects/mvp,Manipulation,,Simulation,,,,,,,実世界の画像をMAEで事前学習して凍結した後、RLでコントローラのみを学習するMVPは、教師あり学習エンコーダよりも最大80%p高い成功率を達成し、一部の課題ではオラクル状態と同等の性能を示します。大規模なHOI画像の活用とPixMCベンチマークの公開により、ピクセルベースのロボット制御研究の新たな基準を提示します。
VLM for Robotics,R3M,R3M: A Universal Visual Representation for Robot Manipulation,CoRL,"Meta, Stanford University",https://arxiv.org/abs/2203.12601,https://sites.google.com/view/robot-r3m/,Manipulation,,"Franka Emika Panda, Simulation",,,,,,,人間の一人称映像と言語を利用して時間・意味・希少性を同時に学習したR3Mは、12個のシミュレーション-5個の実操作課題で、従来の視覚表現に比べて大きな性能向上を示した。汎用ロボット操作用ImageNetに一歩近づいた研究。
End-to-End,Gato,A Generalist Agent,TMLR,Google DeepMind,https://arxiv.org/abs/2205.06175,https://deepmind.google/discover/blog/a-generalist-agent/,"Game, Manipulation",,"Sawyer, Simulation",,,,,,,Atari Games with cross-transformer
VLM for Robotics,VIP,VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training,ICLR,"Meta, University of Pennsylvania",https://arxiv.org/abs/2210.00030,https://sites.google.com/view/vip-rl,Manipulation,,"Franka Emika Panda, Simulation",,,,,,,Temporal Contrative Learningにより学習し、時系列的に近い画像のlatentの距離を近く、時系列的に遠い画像はlatentの距離を遠くし、画像latentによるRL Rewardの生成できるpretrained vision model
"End-to-End, Hierarchical, Prediction",TriVLA,TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control,,China,https://arxiv.org/abs/2507.01424,https://zhenyangliu.github.io/TriVLA/,Manipulation,Tabletop,"Franka Emika Panda, Kinova Gen3, Simulation",Supervised,"CALVIN, LIBERO, MetaWorld","Language, Proprioception, Vision","Open-X-Embodiment, Something-Something","Eagle-2, SigLIP, Stable Video Diffusion",Flow Matching,TriVLAは、Vision-Language Model（VLM）とVideo Diffusion Model（VDM）を統合した3つのシステム構造を備えたロボット制御フレームワークです。複雑な言語命令と物理的なダイナミクスを理解し、長期的な操作タスクを実行できるように設計されています。多様なシミュレーション環境および実環境において最先端の性能を達成し、少ない学習データでも優れた汎化能力を示します。
"End-to-End, Hierarchical",DP-VLA,A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM,CoRL,ETRI,https://arxiv.org/abs/2410.15549,,Manipulation,"Kitchen, Tabletop",Simulation,Supervised,RoboCasa,"Language, Proprioception, Vision",,"CLIP, OpenVLA",Autoregressive,
End-to-End,VQ-VLA,VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers,ICCV,"China, Shanghai Artificial Intelligence Laboratory",https://arxiv.org/abs/2507.01016,https://xiaoxiao0406.github.io/vqvla.github.io/,Manipulation,"Kitchen, Tabletop",Franka Emika Panda,"LoRA, Self-Supervised, Supervised",LIBERO,"Language, Vision","LIBERO, ManiSkill, Open-X-Embodiment, RLBench","OpenVLA, VQ-VAE","Autoregressive, Discrete Token",VQ-VLAは、ロボット操作のためのアクションシーケンスをトークン化するResidual VQ-VAEベースのトークナイザーを提案し、従来の100倍大きなデータセットで学習されることで、より滑らかで長期的な行動生成を可能にします。このトークナイザーは、OpenVLAのようなVision-Language-Actionモデルに統合され、推論速度と性能を向上させます。
"3D, End-to-End",Evo-0,Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding,,"China, Shanghai Jiao Tong University",https://www.arxiv.org/abs/2507.00416,https://github.com/MINT-SJTU/Evo-VLA,Manipulation,Tabletop,xArm,"LoRA, Supervised",,"Language, Proprioception, Vision",,"PaliGemma, VGGT",Flow Matching,Evo-0は、VLA（Vision-Language-Action）モデルに明示的な3D入力を必要とせずに空間理解を強化するため、**Visual Geometry Grounded Transformer (VGGT)**から抽出された幾何学的情報を統合するプラグアンドプレイモジュールを提案します。5つの実際の操作タスクでの実験結果によると、既存のSOTAモデルπ₀と比較して、空間認知および操作性能が大幅に向上しました。
"3D, End-to-End",AC-DiT,AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation,,"China, Nanjing University, Peking University",https://arxiv.org/abs/2507.01961,https://ac-dit.github.io/,"Bi-manual, Manipulation, Move Base","Kitchen, Tabletop",AgileX,"LoRA, Supervised","Maniskill-HAB, RoboTwin","Language, PointCloud, Proprioception, Vision",,"Lift3D, SigLIP",Diffusion,AC-DiTは、モバイルベースとマニピュレーター間の協調性を向上させるために設計された拡散ベースのエンドツーエンドロボット操作モデルです。Mobile-to-Body ConditioningとPerception-aware Multimodal Adaptationという2つの主要なメカニズムを通じて、調整誤差を削減し、段階ごとに適切な視覚情報を動的に活用します。
"Audio, End-to-End",MultiGen,MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real,,UC Berkeley,https://arxiv.org/abs/2507.02864,,Manipulation,Tabletop,Kinova Gen3,"Self-Supervised, Supervised",,"Audio, Language, Proprioception, Vision","EPIC-Kitchens, RoboVerse, Youtube","Audio Spectrogram Transformer, CLIP, MMAudio, SAM2",Diffusion,MultiGenは、シミュレーションベースのロボット政策学習において、ビジョン以外のセンサー（特にオーディオ）を現実的に生成するために、ビデオからオーディオを生成するジェネレーターを物理シミュレーターに統合したフレームワークです。これにより、実際のロボットデータを使用せずに、シミュレーションで学習した政策を実際のマルチセンサーベースのタスク（例：音響ベースの注ぎ作業）にゼロショットで適用することが可能です。
"3D, Dataset, End-to-End",DexVLG,DexVLG: Dexterous Vision-Language-Grasp Model at Scale,ICCV,"China, Peking University",https://arxiv.org/abs/2507.02747,https://jiaweihe.com/dexvlg,"Dexterous Hand, Manipulation",Tabletop,"Shadow Hand, UR10",Supervised,,"Language, PointCloud",DexGraspNet,"Florence-2, SAMesh, Uni3D",Flow Matching,DexVLGは、単一の時点のRGB-D入力と言語指示から器用な（Dexterous）グリップ姿勢を予測するVision-Language-Graspモデルです。このため、1.7億件のグリップポーズとキャプションを含む大規模なデータセットDexGraspNet 3.0を新たに構築しました。
"End-to-End, Humanoid",EgoVLA,EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos,,"MIT, NVIDIA, UC San Diego",https://arxiv.org/abs/2507.12440,https://rchalyang.github.io/EgoVLA/,"Bi-manual, Dexterous Hand, Manipulation",Tabletop,"Simulation, Unitree H1",Supervised,Ego Humanoid Manipulation Benchmark,"Language, Proprioception, Vision","HOI4D, HOT3D, HoloAssist, TACO",NVILA,Autoregressive,EgoVLAは、人間のエゴセントリックな動画から学習したビジョン・言語・行動（VLA）モデルであり、人間の手に動作を予測し、これを二腕型ヒューマノイドロボットに移植して操作を実行します。このため、人間とロボットの行動空間を統一し、少量のロボットデモを用いてその後の微調整を通じて汎用的な操作能力を確保します。
"End-to-End, Tactile",Tactile-VLA,Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization,,"Shanghai Jiao Tong University, Tsinghua University",https://arxiv.org/abs/2507.09160,,"Manipulation, Peg-in-Hole",Assembly,UMI,Supervised,,"Language, Proprioception, Tactile, Vision",VLA-T dataset,"Gemma, Pi0, ViT",Flow Matching,Tactile-VLAは、Vision-Language-Action（VLA）モデルの抽象的な物理知識を触覚入力と組み合わせることで、接触が重要な操作（タスク）において精密な力制御を可能にするフレームワークです。Chain-of-Thoughtに基づく適応的な推論を含むことで、少数のデモだけでも多様な力関連の言語命令に対するゼロショット一般化を実現します。
"Prediction, Special",cVLA,cVLA: Towards Efficient Camera-Space VLAs,,University of Freiburg,https://arxiv.org/abs/2507.02190,,Manipulation,Tabletop,Franka Emika Panda,Supervised,DROID,"Depth, Language, Proprioception, Vision","CLEVR, ManiSkill",PaliGemma,"Autoregressive, Discrete Token",cVLAは、効率的なVision-Language-Action（VLA）学習を実現するため、シミュレーションベースのデータと軽量化されたモデル構造を活用し、カメラ画像フレーム上でロボットのエンドエフェクタのキーポーズを予測するアプローチを提案する。従来の低レベル制御に代わって2つのキーポーズを予測し軌道を構成し、1ステップ先のトークン予測方式を採用することで学習効率を向上させた。
End-to-End,VOTE,VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting,,,https://arxiv.org/abs/2507.05116,https://github.com/LukeLIN-web/VOTE,Manipulation,Tabletop,Simulation,"LoRA, Supervised","LIBERO, SIMPLER","Language, Vision",Open-X-Embodiment,OpenVLA,"Continuous, Parallel Decoding",VOTEはVLMベースのロボット操作モデルであり、アクショントークナイザーを使用せずに直接連続的な行動を予測し、効率的な投票ベースのアンサンブル方式により一般化性能を向上させます。既存のVLAモデルと比較して最大35倍の高速推論速度と低い学習コストを実現しつつ、SOTA性能を達成しています。
End-to-End,VIMA,VIMA: General Robot Manipulation with Multimodal Prompts,ICML,"Caltech, NVIDIA, Stanford University, etc.",https://arxiv.org/abs/2210.03094,https://vimalabs.github.io/,Manipulation,Tabletop,Simulation,Supervised,"RAVENS, VIMA-Bench","Language, Vision",,"T5, ViT","Autoregressive, Discrete Token","マルチモーダルな入力に対応し, 言語や画像で指示ができる. 新ベンチマークVIMA-BENCHの開発, pre-trainedなT5がベース. object detection結果も入れて性能を上げる. decoder onlyではなく, promptのみencoderで処理してからdecoderをした方が性能が良い"
"End-to-End, Open-Source",Octo,Octo: An Open-Source Generalist Robot Policy,RSS,"Carnegie Mellon University, Google DeepMind, Stanford University, UC Berkeley",https://arxiv.org/abs/2405.12213,https://octo-models.github.io/,"Bi-manual, Manipulation","Assembly, Kitchen, Tabletop","ALOHA, Google EDR, UR5, WidowX",Supervised,,"Language, Proprioception, Vision",Open-X-Embodiment,"T5, Transformer, ViT",Diffusion,"汎用ロボットポリシー Octoは80万デモで学習したViT-Diffusionベースのモデルで、9台の実機ロボットでゼロショット制御と100デモの微調整ともにSOTAを更新する。
公開重み付け・コードは、様々なセンサー・アクション空間に素早く適合するロボット「foundation model」の実用化を早める。"
"End-to-End, Open-Source",OpenVLA,OpenVLA: An Open-Source Vision-Language-Action Model,CoRL,"Google DeepMind, MIT, Physical Intelligence, Stanford University, Toyota Research Institute, UC Berkeley",https://arxiv.org/abs/2406.09246,https://openvla.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Google EDR, WidowX","LoRA, Supervised",,"Language, Vision",Open-X-Embodiment,"DINOv2, LLaMA2, SigLIP","Autoregressive, Discrete Token",オープンソースVLA OpenVLAは、97万台のロボットデモでLlama 2 7Bを微調整し、RT-2-Xより小さいながらも高い操作成功率を達成し、LoRA-4-bit量子化により、コンシューマGPUでも簡単に微調整・推論が可能です。つまり、汎用ロボット操作のための軽量で強力なオープンベースのモデルを提供し、研究・応用の拡大を促進します。
End-to-End,CogAct,CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation,,"Microsoft, Tsinghua University",https://arxiv.org/abs/2411.19650,https://cogact.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Realman, Simulation",Supervised,SIMPLER,"Language, Proprioception, Vision",Open-X-Embodiment,"DINOv2, LLaMA2, SigLIP",Diffusion,CogACTは、VLMベースの認知とDiTベースの行動モジュールを分離・結合し、ロボット操作性能と一般化を大幅に向上させました。OXEの事前学習後、少量の微調整だけでシミュレーション・実世界ともに従来のVLA(特にRT-2-X-OpenVLA)に比べて最大55 pp↑の成功率を達成する。
End-to-End,RDT-1B,RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation,ICLR,Tsinghua University,https://arxiv.org/abs/2410.07864,https://rdt-robotics.github.io/rdt-robotics/,"Bi-manual, Manipulation",Tabletop,ALOHA,Supervised,,"Language, Proprioception, Vision","DROID, Mobile ALOHA Dataset, Open-X-Embodiment, RH20T, RT-1","SigLIP, T5",Diffusion,RDT-1Bは、統合行動空間+Diffusion Transformerで46個のロボットデータから事前学習した1.2B規模の両腕操作基盤モデルです。実験的に、零/少数のショット一般化と高度の繊細な操作まで、SOTAを大きく上回りました。
End-to-End,RoboMamba,RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation,NeurIPS,Peking University,https://arxiv.org/abs/2406.04339,https://sites.google.com/view/robomamba-web,Manipulation,"Kitchen, Tabletop",Franka Emika Panda,Supervised,SAPIEN,"Language, Vision","LLaVA Instruction, LLaVA-LCS, LLaVA-Next, RoboVQA, ShareGPT4V-SFT","CLIP, Mamba",Continuous,Mamba言語モデルとCLIPを組み合わせて作られたRoboMambaは、推論力と効率を同時に備えたVLAで、0.1%の微調整でSOTA操作性能と3倍の高速制御周波数を達成する。つまり、「小さく学習して大きく動く」ロボット用マルチモーダルコアエンジンである。
"Affordance, End-to-End",Chain-of-Affordance,Improving Vision-Language-Action Models via Chain-of-Affordance,,Shanghai University,https://arxiv.org/abs/2412.20451,https://chain-of-affordance.github.io/,Manipulation,"Kitchen, Tabletop",Franka Emika Panda,"LoRA, Supervised",LIBERO,"Language, Vision",DROID,"DiffusionVLA, Qwen2-VL",Diffusion,ロボットがObject→Grasp→Spatial→Movementのアフォーダンスで段階的に推論して行動するように設計されたChain-of-Affordanceが、現実とシミュレーションの両方で従来のVLAベースのモデルを大きく上回りました。少ないデータと軽量のバックボーンでも、複雑な作業、障害物、見慣れない物体の姿勢でタフネスを確保した点が重要な貢献です。
End-to-End,Pi-0,π_0: A Vision-Language-Action Flow Model for General Robot Control,,Physical Intelligence,https://arxiv.org/abs/2410.24164,https://www.physicalintelligence.company/blog/pi0,"Bi-manual, Manipulation, Move Base","Kitchen, Tabletop","ALOHA, AgileX, Fibocom, Franka Emika Panda, UR5",Supervised,,"Language, Proprioception, Vision","BridgeV2, DROID, Open-X-Embodiment",PaliGemma,Flow Matching,π0は、VLMをベースにFlow Matching Action Expertを組み合わせ、1万時間以上のマルチロボットデータを事前学習した汎用ロボットファウンデーションモデルです。実験では、ゼロ・ショット・少量微調整ともに従来モデルを大きく上回り、洗濯物の折り畳み・箱詰めなどの長期高難度作業も実行します。
End-to-End,Pi-0.5,Pi-0.5: a Vision-Language-Action Model with Open-World Generalization,,Physical Intelligence,https://arxiv.org/abs/2504.16054,https://www.pi.website/blog/pi05,"Bi-manual, Manipulation, Move Base","Kitchen, Tabletop",,Supervised,,"Language, Proprioception, Vision","COCO, Cambrian-7M, CapsFusion, PixMO, VQAv2","Gemma, SigLIP","Autoregressive, Flow Matching",π0.5は、Web・複数のロボット・言語デモを一緒に学習し、少ないモバイルデモだけで見知らぬ家の掃除を長時間行う階層型VLAモデルです。FAST TokenとFlow Matchingアクションエキスパートとの組み合わせにより、π0に比べて一般化性能を大幅に向上させた。
End-to-End,Pi-0-FAST,FAST: Efficient Action Tokenization for Vision-Language-Action Models,,"Physical Intelligence, Stanford University, UC Berkeley",https://arxiv.org/abs/2501.09747,https://www.pi.website/research/fast,"Bi-manual, Manipulation","Kitchen, Tabletop","ALOHA, AgileX, Franka Emika Panda, Simulation, UR5",Supervised,LIBERO,"Language, Proprioception, Vision","BridgeV2, DROID, LIBERO, RT-1, RT-2","DINOv2, LLaMA2, PaliGemma, SigLIP","Autoregressive, Discrete Token",DCT (Discrete Cosine Transform) + BPE (Byte-Pair Encoding)でアクションを圧縮するFASTは、高周波のロボット操作でも情報量の多いトークン列を作成し、汎用ボーカFAST+と組み合わせることで、デクスターラス-大規模データ学習を最大5倍高速化します。これにより、π0-FASTは従来のDiffusion VLAと同等の性能を維持しながら、学習・ゼロショット一般化が大幅に向上します。
End-to-End,OpenVLA-OFT,Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success,RSS,Stanford University,https://arxiv.org/abs/2502.19645,https://openvla-oft.github.io/,"Bi-manual, Manipulation",Tabletop,ALOHA,Supervised,LIBERO,"Language, Proprioception, Vision",Open-X-Embodiment,"DINOv2, LLaMA2, SigLIP","Continuous, Parallel Decoding",並列デコード-アクションチャンキング-連続＋L1回帰で構成されるOFTレシピは、VLAファインチューニングの速度・性能・拡張性を同時に向上させ、シミュ-実ロボットで最新の方法を凌駕します。
End-to-End,HybridVLA,HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model,,Peking University,https://arxiv.org/abs/2503.10631,https://hybrid-vla.github.io/,"Bi-manual, Manipulation",Tabletop,"Franka Emika Panda, Simulation",Supervised,RLBench,"Language, Proprioception, Vision","DROID, Open-X-Embodiment, ROBOMIND","CLIP, DINOv2, LLaMA2, Phi2, SigLIP","Autoregressive, Diffusion",拡散と自己回帰を一つのLLMの中に協調的に統合し、連続制御とセマンティック推論の両方を活かしたロボットVLAモデルを提案する。RLBench-実世界の片腕・両腕タスクにおいて、従来のSOTAを14-33%上回り、速度と精度のトレードオフのためにディフュージョンのみのバリエーションも提示する。
End-to-End,NORA,NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks,,Singapore University of Technology and Design,https://arxiv.org/abs/2504.19854,https://declare-lab.github.io/nora,Manipulation,Tabletop,WidowX,Supervised,LIBERO,"Language, Vision",Open-X-Embodiment,Qwen2.5-VL,"Autoregressive, Discrete Token",NORAはQwen-2.5-VLベースの3 B-parameter軽量VLAで、FAST+トルクナイザーと97万台のロボットデモ学習により、リアルタイム推論・高い成功率を同時に達成する。既存の7 B以上のモデルより▲演算効率▲OOD・空間・推論性能で先行し、NORA-LONGはaction chunkingで長期計画まで拡張した。
End-to-End,SpatialVLA,SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model,RSS,"Shanghai Jiao Tong University, Zhejiang University",https://arxiv.org/abs/2501.15830,https://spatialvla.github.io/,Manipulation,"Kitchen, Tabletop","Franka Emika Panda, WidowX","LoRA, Supervised","LIBERO, SIMPLER","Depth, Language, Vision","BridgeV2, Open-X-Embodiment, RH20T","PaliGemma, SigLIP, ZoeDepth",Autoregressive,Ego3DエンコーディングとAdaptive Action Gridsで3D空間認識を組み合わせたSpatialVLAは、わずか3つのトークンで行動を生成し、110万シナリオの事前学習後、様々なロボット・作業でSOTA性能と迅速な適応を実証しました。
End-to-End,MoLe-VLA,MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation,,"HKUST, Nanjing University, Peking University",https://arxiv.org/abs/2503.20384,https://sites.google.com/view/mole-vla,Manipulation,Tabletop,Franka Emika Panda,"Distillation, Supervised",RLBench,"Language, Vision",,"DINOv2, LLaMA2, SigLIP",Diffusion,STAR (Spatial-Temporal Aware Router)で必要なレイヤーだけを選んで書き、CogKD (self-knowledge distillation)で失われた認知力を補い、計算量を最大5.6倍削減しながら、ロボット操作性能を平均8％向上させたVLAモデル。 つまり、「脳のように浅く、選択的に考える」効率型ビジョン・言語・行動モデルです。
End-to-End,UP-VLA,UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent,ICML,Tsinghua University,https://arxiv.org/abs/2501.18867,https://github.com/CladernyJorn/UP-VLA,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,CALVIN,"Language, Vision","BridgeV2, LLaVA Instruction","CLIP, Phi1.5",Autoregressive,"UP-VLAは、CLIP-ViT+VQ-GANエンコーダと1.3B Phi-1.5ベースのLLMを組み合わせて、画像-テキスト-アクショントークンを一つのAuto-regressive ストリームとして生成し、MMU(質疑応答)、PRE(未来フレーム予測)、Action(ロボットシーケンス)の3つの課題をアテンションマスクのみを変えて同時学習する。
Bridge-v2ロボットデモ(25 k)-LLaVA-Tuning(665 k)事前学習後、CALVIN+実物把持データを共同ファインチューニングし、CALVIN ABC→D成功率33%↑、実世界未知物体把持8pt↑でSOTAを更新した。"
End-to-End,DexVLA,DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control,,Shanghai University,https://arxiv.org/abs/2502.05855,https://dex-vla.github.io/,"Bi-manual, Dexterous Hand, Manipulation","Kitchen, Tabletop","AgileX, Franka Emika Panda, Inspire Hand, UR5",Supervised,LIBERO,"Language, Vision",,Qwen2-VL,Diffusion,"DexVLAは、マルチヘッド10億パラメータ拡散アクションエキスパートと3段階のEmbodied Curriculum(汎用事前学習→ロボット別微調整→少量・長期適応)手法で、多種のロボットの操作政策をデータ効率的に学習する。
また、Sub-step推論文生成+Reasoning-to-FiLM注入により、高レベルプランナーがなくても洗濯物開梱のような長期課題を安定的に実行し、従来のVLA-Diffusion Policyを上回る。"
End-to-End,ObjectVLA,ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration,,Shanghai University,https://arxiv.org/abs/2502.19250,https://objectvla.github.io/,Manipulation,Tabletop,Franka Emika Panda,Supervised,,"Language, Vision",,"DiffusionVLA, Qwen2-VL",Diffusion,"Bounding-box位置が注釈された画像-テキスト(ロボット:画像=10:1)をDiVLA-2Bと共同学習し、言語↔行動位置-groundingを形成、デモなしで100個のOODオブジェクトで64%の操作に成功。
携帯電話の写真21枚で1-epoch微調整し、新しいオブジェクトも10分以内に拡張、低コストデータだけで汎用オブジェクト操作を実現したObjectVLAフレームワーク。"
"End-to-End, Hierarchical, Humanoid",Gr00t-N1,GR00T N1: An Open Foundation Model for Generalist Humanoid Robots,,NVIDIA,https://arxiv.org/abs/2503.14734,https://research.nvidia.com/publication/2025-03_nvidia-isaac-gr00t-n1-open-foundation-model-humanoid-robots,"Bi-manual, Dexterous Hand, Manipulation","Kitchen, Tabletop",GR-1,Supervised,"DexMimigGen, RoboCasa","Language, Proprioception, Vision","Assembly101, DexMimicGen, EPIC-Kitchens, Ego-Exo4D, Ego4D, GR00T, HOI4D, HoloAssist, Open-X-Embodiment, RH20T","Eagle-2, SigLIP, VQ-VAE",Flow Matching,"デュアル-システム Eagle-2 VLM (推論) + DiT Flow-Matching Policy (行動) 構造とWeb-シミュ-実ロボットピラミッドデータを統合学習し、1つのモデルがマルチロボットに言語条件操作を高頻度で行う。
シミュレーション3種・実機GR-1評価で、従来のBC-Transformer/Diffusion Policyと比較して最大30%p以上の性能を上げ、10%のデモだけで同様の性能を達成した。"
End-to-End,TinyVLA,"TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation",RAL,Shanghai University,https://arxiv.org/abs/2409.12514,https://tiny-vla.github.io/,"Bi-manual, Manipulation",Tabletop,"Franka Emika Panda, UR5","LoRA, Supervised",MetaWorld,"Language, Proprioception, Vision",,"Pythia, ViT",Diffusion,小さなVLM(≤1.3B)にLoRAのみを学習し、Diffusion Policyでアクションを直接サンプリングするTinyVLAは、事前ロボットデータがなくてもOpenVLAより20倍速く、成功率が高い→軽量VLM + LoRA +拡散ヘッドで「高速・データ効率・汎用性」を同時に達成したVLAモデル。
End-to-End,ChatVLA,ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model,,"Shanghai University, Tsinghua University",https://arxiv.org/abs/2502.14420,https://chatvla.github.io/,Manipulation,"Kitchen, Tabletop",Franka Emika Panda,Supervised,,"Language, Vision","COCO, GQA, OCR-VQA, TextVQA, VisualGenome",Qwen2-VL,Diffusion,"Phased Alignment TrainingとMoE構造により、視覚・言語理解とロボット制御を1つの2 Bモデルに統合し、理解ベンチマーク(MMMU 6×↑)と25個の実ロボットタスクの両方でSOTA VLAを上回った。
→ Spurious forgetting・タスク干渉を解決した軽量ユニファイドVLAの実用可能性を提示。"
"3D, End-to-End",PointVLA,PointVLA: Injecting the 3D World into Vision-Language-Action Models,,Shanghai University,https://arxiv.org/abs/2503.07511,https://pointvla.github.io/,"Bi-manual, Manipulation",Tabletop,"AgileX, UR5",Supervised,RoboTwin,"Language, PointCloud, Vision",,Qwen2-VL,Diffusion,PointVLAは、事前学習されたVLAのアクションブロックのうち重要度の低い5つの層に点群特徴を残差として注入し、3D空間情報を追加しながら2D表現を保持する。この軽量な手法のおかげで、少数のデモでも、多課題・長距離・高さ変化・実物区別で従来の2D-3Dポリシーを圧倒する。
,VLA-Cache,VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation,,University of Sydney,https://arxiv.org/abs/2502.02175,,Manipulation,,"Kinova jaco, Simulation",,"LIBERO, SIMPLER",,,,,VLA-Cacheは、連続フレームの静的な視覚トークンをキャッシュし、タスクに重要なトークンのみを再計算する層別適応型手法で、学習を変更することなくVLA推論を最大1.7倍高速化します。実験の結果、LIBERO-SIMPLER・実ロボットともに、成功率の維持・向上とともに、FLOPs・遅延を大幅に削減しました。
End-to-End,DiffusionVLA,Diffusion-VLA: Generalizable and Interpretable Robot Foundation Model via Self-Generated Reasoning,ICML,Shanghai University,https://arxiv.org/abs/2412.03293,https://diffusion-vla.github.io/,"Bi-manual, Manipulation","Industrial, Tabletop","Franka Emika Panda, UR5","LoRA, Supervised",,"Language, Vision","DROID, Open-X-Embodiment","Qwen2-VL, SigLIP",Diffusion,"手法 - Qwen2-VLの自己chain-of-thoughtをFiLM注入Diffusion Policyと組み合わせ、推論可能な高周波制御を一度に学習。
成果 - 実機Franka-両腕ロボットで最大82 Hzで動作し、工場仕分け-102種bin-pickなどで従来VLAより+20 - 35 p成功率向上。"
,Interleave-VLA,Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions,,"Shanghai Jiao Tong University, UC Berkeley",https://arxiv.org/abs/2505.02152,,Manipulation,,"FANUC Mate, Simulation",,,,,,,"テキスト指示のみを受けていたVLAに画像-テキストのインターリーブ入力をサポートするように最小限修正したInterleave-VLAを提案し、21万エピソード規模のOpen Interleaved X-Embodimentで学習し、
その結果、シミュレータ-実ロボットで従来モデルに比べて2-3倍高いout-of-domain性能とスケッチ-ウェブ画像ゼロショット動作を達成した。"
"End-to-End, Hierarchical",RT-H,RT-H: Action Hierarchies Using Language,RSS,"Google DeepMind, Stanford University",https://arxiv.org/abs/2403.01823,https://rt-hierarchy.github.io/,Manipulation,"Kitchen, Tabletop",Google EDR,Supervised,,"Language, Vision","RT-1, RT-2","PaLI-X, ViT","Autoregressive, Discrete Token","言語モーションを中間層に置くRT-Hは、「課題→言語モーション→行動」の2段階Transformerを学習し、課題間の構造を共有する。
自動ラベル付けされた2500以上のモーションで、RT-2と比較して平均15％の性能を上げ、言語補正だけでさらに23％の改善・強力な一般化を達成した。"
Special,AutoRT,AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents,,Google DeepMind,https://arxiv.org/abs/2401.12963,https://auto-rt.github.io/,Manipulation,"Kitchen, Tabletop",Google EDR,Supervised,,"Language, Vision",,"FlexCap, PaLI",,"VLMベースのオープンボーカブラリーオブジェクト検出とシーンキャプションをLLMに入れ、課題を生成し、「ロボット憲法」プロンプトで危険・不可タスクをフィルタリングした後、Pick-Teleop-RT-2ポリシーを確率的に実行し、7ヶ月間、20台以上のロボットで77 kの実世界エピソードを自動収集した。
この多様・安全データでRT-1を再学習すると、高低一般化Pick成功率が0→12.5％、Wipeが10→30％に向上した。"
Special,Sara-RT,SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust Attention,ICRA,Google,https://arxiv.org/abs/2312.01990,https://sites.google.com/view/rtsara/,Manipulation,Tabletop,"Google EDR, Kuka iiwa","Blackbox Optimization, Supervised",Matterport3D,"Language, PointCloud, Vision","RT-1, RT-2","PaLI-X, ViT","Autoregressive, Discrete Token","SARA-RTは学習型Gaussian projection+ReLU/exp kernelでソフトマックスアテンションを線形化するSelf-Adaptive Robust Attention up-training技法を提案、RT-2 5B-PCTで最大14%速度↑・精度↑・維持。
核心手法: 一度のアップトレーニングで従来のクアドラティックRTをm=d線形アテンション(ϕ<sub>SARA</sub>)ポリシーに変換し、ロボット実技推論を~100 msレベルに短縮。"
"End-to-End, VLM for Robotics",Gemini Robotics,Gemini Robotics: Bringing AI into the Physical World,,Google DeepMind,https://arxiv.org/abs/2503.20020,https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/,"Bi-manual, Manipulation","Kitchen, Tabletop","ALOHA, Franka Emika Panda","Self-Supervised, Supervised","BLINK, ERQA, RealWorldQA","Language, Vision","EGTEA Gaze+, ERQA, HoloAssist, MECCANO, Open-X-Embodiment, UMI",Gemini,"Continuous, Primitive","Gemini Roboticsは、Gemini 2.0ベースのVLMをER微調整→クラウドバックボーン＋オンボードデコーダVLAに拡張し、3D認知・コード生成・ICL・大規模テレオペレータデータ学習でロボットをゼロショット-few-shot制御し、安全性までカバーする汎用ロボットモデルです。
ERQA-ALOHA 2-長期特化・新プラットフォームの実験で、従来のπ0-ディフュージョンポリシーを上回り、長い軌跡・言語・視覚・行動の一般化をすべて達成する。"
End-to-End,ICRT,In-Context Imitation Learning via Next-Token Prediction,ICRA,UC Berkeley,https://arxiv.org/abs/2408.15980,https://icrt.dev/,Manipulation,Tabletop,Franka Emika Panda,Supervised,,"Proprioception, Vision",DROID,"Transformer, ViT",Autoregressive,"プロンプトで与えられたデモシーケンスを文脈として、次のトークン予測のみを用いて新しいタスクを即座に実行するロボットポリシーICRTを提案する。
マルチタスクデータとプロンプト後損失設計により、Frankaロボットの実験においてOcto・OpenVLAと比較して最大15倍の高い成功率を達成した。"
"Cross-embodiment, End-to-End",UniAct,Universal Actions for Enhanced Embodied Foundation Models,CVPR,"Peking University, Tsinghua University",https://arxiv.org/abs/2501.10105,https://2toinf.github.io/UniAct/,"Bi-manual, Manipulation",Tabletop,"AIRBOT, Simulation, WidowX",Supervised,LIBERO,"Language, Vision","BC-Z, BridgeV2, DROID, LIBERO, Language Table, Open-X-Embodiment",LLaVA-OneVision,Discrete Token,"UniActは、VLMが選択するベクトル量子化ユニバーサルアクションスペースとロボットごとのMLPデコーダーにより、アクションの異質性を根本的に解決しました。28台のロボットと100万件のデモを0.5Bパラメーターで学習し、14倍の大型モデルを超える汎用操作性能を示しました。
手法：Gumbel-Softmaxでコードブック（256×128）の原子行動を抽出するとともに、軽量ヘッド/behavior-cloningにより多様なロボットに迅速に適応します。"
"3D, End-to-End, Prediction",3D-VLA,3D-VLA: A 3D Vision-Language-Action Generative World Model,ICML,"MIT, Shanghai Jiao Tong University",https://arxiv.org/abs/2403.09631,https://github.com/UMass-Embodied-AGI/3D-VLA,Manipulation,Tabletop,Simulation,"LoRA, Supervised","CALVIN, RLBench","Depth, Language, PointCloud, Vision","BC-Z, BridgeV2, CALVIN, Dobb-E, EPIC-Kitchens, Fractal, HOI4D, Jaco Play, Language Table, Mutex, Open-X-Embodiment, RH20T, RLBench, RT-1, RoboVQA, Roboturk",BLIP-2,"Autoregressive, Discrete Token",3D-LLMのバックボーンにオブジェクト・位置・行動トークンを注入し、RGB-D/ポイントクラウド目標を生成するディフュージョン世界モデルを組み合わせることで、3次元ワールドモデルを実装。 ➋ 2Mの3D言語行動データで学習し、3D推論・目標想像・ロボット計画のすべてでSOTAを凌駕した。
End-to-End,CoT-VLA,CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models,CVPR,"NVIDIA, Stanford University",https://arxiv.org/abs/2503.22020,https://cot-vla.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Simulation, WidowX",Supervised,LIBERO,"Language, Vision","EPIC-Kitchens, Open-X-Embodiment, Something-Something",VILA-U,Discrete Token,CoT-VLAは、サブゴール画像→アクションチャンク2段階・ハイブリッドアテンション手法を用いて視覚的チェーン・オブ・シンキング推論を実現した7B VLAです。この手法は、OpenX + 非行動動画事前学習を通じて、実ロボットで17%、シミュレーションで6%の性能向上を実現しました。
"End-to-End, Hierarchical, RL",SLIM,SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon Visuomotor Learning,,Horizon Robotics,https://arxiv.org/abs/2501.09905,,"Manipulation, Navigation, Whole-Body Control","Indoor, Outdoor","Unitree Go1, WidowX","Distillation, RL",,"Language, Proprioception, Vision",,,Continuous,著者らは、Go1 + WidowXにおいてサブタスク分解 + PEX教師-生徒SAC/PPO階層RLと視覚ボトルネック・ドメインランダム化により、シミュレーションのみで学習したポリシーが実環境で80％に近い長期投下タスクを達成したことを示した。①低レベル速度追従歩行、②高レベル言語・視覚生徒ポリシー、③必須ダイナミクス‒ビジョンランダム化が核心手法である。
End-to-End,SafeVLA,SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning,,Peking University,https://arxiv.org/abs/2503.03480,https://pku-safevla.github.io/,"Manipulation, Move Base, Navigation","Indoor, Kitchen",Simulation,"RL, Supervised",AI2THOR,"Language, Proprioception, Vision",,"DINOv2, EmbCLIP, SigLIP","Autoregressive, Discrete Token","安全予測をコストに変換した後、ラグランジュSafeRLで学習する**ISAパイプライン（モデリング・リスク誘導・制約学習・検証）**を提案し、Safety-CHORESでリスク83.6%削減・成功率+3.9%を達成しました。
核心手法は、CMDPに基づくコスト制約＋リスク多様化データ＋適応的ラグランジュ最適化により、安全性と性能を同時に確保することである。"
End-to-End,Deer-VLA,DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution,NeurIPS,"ByteDance Research, Tsinghua University",https://arxiv.org/abs/2411.02359,https://github.com/yueyang130/DeeR-VLA,Manipulation,Tabletop,Simulation,Supervised,CALVIN,"Language, Vision",CALVIN,"CLIP, LSTM, OpenFlamingo","Autoregressive, Continuous","動的Early-Exitマルチ-exit MLLM（Flamingo）とアクション一貫性に基づく終了基準により、状況に応じて必要な層まで実行することで、FLOPs・メモリを最大6.5倍/6倍削減しつつ、CALVINの操作性能を維持しました。
閾値はデータ推定・ベイジアン最適化により、予算内で自動的に調整されます。"
End-to-End,LAPA,Latent Action Pretraining from Videos,ICLR,"Allen Institute for AI, KAIST, Microsoft, NVIDIA, University of Washington",https://arxiv.org/abs/2410.11758,https://latentactionpretraining.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Simulation, WidowX","Self-Supervised, Supervised","Language Table, SIMPLER","Language, Vision","BridgeV2, Open-X-Embodiment, Something-Something",LWM-Chat,Discrete Token,"VQ-VAEで得られた離散行動トークンを予測するようにVLMを非監督事前学習し、少量ラベルでデコードのみ微調整するLAPAは、ウェブ動画のみでもOpenVLA比で+6.2%の性能向上と30倍の高速学習を実現。
つまり、行動ラベルのないインターネット規模の動画→トークン化→VLM BC→少量ロボット微調整というシンプルな手法で、汎用ロボット基盤モデルを構築する。"
"3D, End-to-End",FP3,FP3: A 3D Foundation Policy for Robotic Manipulation,,"Tsinghua University, UC San Diego",https://arxiv.org/abs/2503.08950,https://3d-foundation-policy.github.io/,Manipulation,Tabletop,Franka Emika Panda,Supervised,,"Language, PointCloud, Proprioception",DROID,"CLIP, Transformer, Uni3D",Diffusion,"事前訓練された1.3B Diffusion Transformerがポイントクラウド＋言語を直接入力し、60kシナリオの学習後、LoRAを用いて80回のデモで4つのロボット作業において、ドメイン内95％、ドメイン外83％の成功率を達成しました。
核心手法は、Uni3D ViTベースの3Dエンコーダー＋adaLNディフュージョン・トランスフォーマーと、事前学習→少量LoRA微調整により、データ効率と汎化性能を同時に確保した点にある。"
End-to-End,,VLA Model-Expert Collaboration for Bi-directional Manipulation Learning,,China,https://arxiv.org/abs/2503.04163,https://aoqunjin.github.io/Expert-VLA/,Manipulation,Tabletop,Simulation,Supervised,MetaWorld,"Language, Vision",MetaWorld,,,"専門家が各段階ごとにVLAモデルがN段階を担当する交代制御と、そのデータでVLAを再学習する双方向協力フレームワークを提案し、MetaWorldおよびBCI実験において成功率の向上と専門家の負担軽減を実証しました。
手法：VLA-Expert N:1協業（推奨N=4）→協業データバッファリング→監督学習に基づく微調整→反復によりモデルと専門家双方の性能向上。"
End-to-End,MoManipVLA,MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation,,"Nanyang Technological University, Tsinghua University",https://arxiv.org/abs/2503.13446,,"Manipulation, Move Base","Indoor, Tabletop","Hello Stretch, RM65, Simulation",Supervised,OVMM,"Depth, Language, Proprioception, Vision",OVMM,OpenVLA,Optimization,"VLAが予測したエンドエフェクタのウェイポイントを、ベースとアームの二重最適化（到達性・滑らかさ・衝突回避）により実行するMoManipVLAを提案し、OVMM/実環境においてSOTA比で成功率4.2％ポイント向上を実現しました。
50件の実演のみでゼロショットレベルのモバイル操作汎用性を達成し、モバイルマニピュレーションの汎用性を大幅に向上させました。"
End-to-End,RoboFlamingo-Plus,RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation,,,https://arxiv.org/abs/2503.19510,https://roboflamingo.github.io/,Manipulation,Tabletop,Franka Emika Panda,Supervised,CALVIN,"Depth, Language, Proprioception, Vision",CALVIN,"OpenFlamingo, ViT","Autoregressive, Continuous","RoboFlamingo-Plusは、ViT-Perceiver Resamplerを使用してRGB-Depthを融合し、言語条件交差アテンション・LSTMポリシーヘッドでロボットの行動を予測することで、CALVINベンチマークにおいて**成功率を10～20％**改善しました。
深度正規化・Resamplerの微調整により、未知の環境や変形した言語指示においても堅牢な性能を発揮します。"
End-to-End,PD-VLA,Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding,,"HKUST, Zhejiang University",https://arxiv.org/abs/2503.02310,,Manipulation,Tabletop,"Simulation, Unitree Z1-Pro",Supervised,CALVIN,"Language, Vision",CALVIN,"CLIP, LLaVA, Vicuna","Discrete Token, Parallel Decoding","ジャコビ固定点並列デコードにより、行動チャンキングVLAのARボトルネックを解消し、学習なしで制御周波数2.5倍・成功率向上を実現。
PD-VLA：モデル修正なし・訓練なしにより、シミュレーションと実環境の両方で速度と性能の同時改善を達成。"
"End-to-End, Hierarchical",Hi Robot,Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models,,Physical Intelligence,https://arxiv.org/abs/2502.19417,https://www.pi.website/research/hirobot,"Bi-manual, Manipulation, Move Base","Kitchen, Tabletop","ALOHA, AgileX, UR5",Supervised,,"Language, Proprioception, Vision",,PaliGemma,Flow Matching,階層型VLM→VLA構造と合成相互作用データを用いて、高次指示・リアルタイムフィードバックをロボット行動に変換する「Hi Robot」を提案する。3種類のロボット×長距離作業において、GPT-4o・Flat VLAよりも指示の一致率・課題の成功率を大幅に改善した。
VLM for Robotics,Unified-IO 2,"Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action",CVPR,"Allen Institute for AI, University of Illinois Urbana-Champaign, University of Washington",https://arxiv.org/abs/2312.17172,https://unified-io-2.allenai.org/,"Manipulation, Navigation",Indoor,Simulation,Supervised,"Habitat, VIMA-Bench","Audio, Depth, Language, Vision",,ViT,,"各モダリティ（テキスト・画像・音声・アクション）をすべてトークン化して単一のエンコーダー・デコーダー・トランスフォーマーで処理し、2D RoPE・QK正規化・スケーラード・コサイン・アテンション・マルチモーダル・ミクスチャー・オブ・デノイザーズ・ダイナミック・パッキングなど、安定化手法を導入。
7 B Unified-IO 2は画像・音声・行動の生成までサポートし、GRIT SOTAを含む35のベンチマークで汎用マルチモーダル能力を実証しました。"
"End-to-End, Hierarchical",HAMSTER,HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation,ICLR,"NVIDIA, University of Southern California, University of Washington",https://arxiv.org/abs/2502.05485,https://hamster-robot.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,Colosseum,"Depth, Language, PointCloud, Proprioception, Vision","BridgeV2, DROID, LLaVA Instruction, RLBench, RoboPoint",VILA,"Autoregressive, Continuous","① 上位層のVILA-13Bが2次元経路を、下位層のRVT-2/3D-DAが3次元動作を実行する階層型VLA「HAMSTER」を提案し、オフドメインの映像・シミュレーション・既存のロボットデータで学習。
② 実ロボットの7軸一般化において、OpenVLAと比較して+20 %-p（50 %向上）・デモの½でも2倍の性能を達成し、Colosseumシミュレーションでは+31 %を達成しました。"
"Audio, End-to-End, Tactile",FuSe,Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding,ICRA,UC Berkeley,https://arxiv.org/abs/2501.04693,https://fuse-model.github.io/,Manipulation,Tabletop,WidowX,"Self-Supervised, Supervised",,"Audio, Language, Proprioception, Tactile, Vision",,"Octo, PaliGemma","Autoregressive, Continuous","自然言語の対照と生成損失を組み合わせ、視覚ベースの汎用ポリシーを触覚・音響を含むマルチセンサーでファインチューニング（FuSe）→ 27,000件の実ロボットデータで成功率20%向上、マルチセンサー・合成プロンプトをゼロショット実行。"
End-to-End,OTTER,OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction,ICML,"Meta, UC Berkeley",https://arxiv.org/abs/2503.03734,https://ottervla.github.io/,Manipulation,Tabletop,Franka Emika Panda,Supervised,LIBERO,"Language, Proprioception, Vision",LIBERO,"CLIP, Transformer","Autoregressive, Continuous","手法: CLIPを凍結後、言語データからパッチ類似度に基づく「テキスト認識視覚トークン」のみを抽出し、Transformerモデルに入力。
2行要約: 凍結VLMとテキスト認識視覚特徴を組み合わせることで、ロボット動作予測の精度向上を実現。実ロボットとシミュレーションのゼロショット環境において、従来のVLAと比較して最大約70%の成功率向上を達成。"
"End-to-End, RL, Training",iRe-VLA,Improving Vision-Language-Action Model with Online Reinforcement Learning,,"Tsinghua University, UC Berkeley",https://arxiv.org/abs/2501.16664,,Manipulation,Tabletop,"Franka Emika Panda, Simulation","LoRA, RL, Supervised","FrankaKitchen, MetaWorld","Language, Vision","FrankaKitchen, MetaWorld",BLIP-2,Continuous,iRe-VLAは、BLIP-2ベースのVLAにLoRAを適用し、オンラインRL（アクション・ヘッドマン学習）と成功軌跡の教師付き学習を交互に実行する2段階反復法により、学習の不安定性を解消します。この手法は、MetaWorld・FrankaKitchen・実際のPanda実験において、従来のSFT・PPOと比較して最大+40%の成功率向上と未実施タスクの一般化性能向上を実現しました。
"Hierarchical, Planning",Bi-VLA,Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations,,Russia,https://arxiv.org/abs/2405.06039,,Manipulation,,UR3,,,,,,,"RAG+Starling-LM-plannerとQwen-VLの検出結果をAPIマッピングし、2台のUR3アームがサラダ調理動作（盛り付け・切り分け・混ぜ合わせなど）を自動実行するVi-Lang-Act統合フレームワーク。
言語100％、視覚96％、全体83％の精度で複数のサラダタスクを実行し、視覚検出の誤差が主な限界である。"
End-to-End,GraspVLA,GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data,,"Peking University, The University of Hong Kong",https://arxiv.org/abs/2505.03233,https://pku-epic.github.io/GraspVLA-web/,"Grasp, Manipulation",Tabletop,Franka Emika Panda,Supervised,LIBERO,"Language, Proprioception, Vision",SynGrasp-1B,"DINOv2, InternLM2, SigLIP",Flow Matching,合成グリップデータSynGrasp-1B（10億フレーム）とウェブグラウンドデータを用いて、**PAGチェーン・オブ・ソート（bbox → グリップポーズ → フローマッチングアクション）**で学習したGraspVLAは、**実物ゼロショットオープンボキャブラリグリップ90%+**とバウンディングボックスのみによるフューショット適応を実現しました。
"End-to-End, RL",ConRFT,ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy,RSS,China,https://arxiv.org/abs/2502.05450,https://cccedric.github.io/conrft/,Manipulation,Tabletop,Franka Emika Panda,RL,,"Language, Proprioception, Vision",,"Kosmos-2, PaliGemma",Diffusion,ConRFTは、BC + Cal-QLベースのオフライン初期化とヒューマン・インターベンション一貫性RLを組み合わせることで、少ないデモデータのみでVLAモデルを安全かつ高効率に強化する手法です。8つの実際のタスクにおいて96.3%の成功率（従来比+144%）を達成し、RLベースのファインチューニングの実用性を実証しました。
"End-to-End, Training",,"Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better",,Physical Intelligence,https://arxiv.org/abs/2505.23705,https://www.pi.website/research/knowledge_insulation,"Bi-manual, Manipulation, Move Base",Tabletop,"ALOHA, AgileX, Fibocom, Franka Emika Panda, Simulation, UR5",Supervised,"DROID, LIBERO","Language, Proprioception, Vision","DROID, LIBERO",PaliGemma,"Autoregressive, Discrete Token, Flow Matching","継続的にFAST離散トークンとフローマッチング連続エキスパートを同時に学習するが、エキスパート→バックボーン経路を遮断してVLMの知識を保持するKnowledge Insulation VLAを提案する。
その結果、学習と推論の両方が高速化され、従来のVLAに比べて汎化性能、言語追従性、成功率が大幅に向上する。"
"End-to-End, Humanoid",AgiBot World Colosseo,AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems,IROS,Team AgiBot-World,https://arxiv.org/abs/2503.06669,https://opendrivelab.com/AgiBot-World/,"Bi-manual, Dexterous Hand, Manipulation","Industrial, Kitchen, Tabletop",AgiBot G1,"Self-Supervised, Supervised",,"Language, Proprioception, Vision",AgiBot World,"InternVL2.5, VQ-VAE",Diffusion,"現実世界における100万件以上のトラジェクトリーを収録したAgiBot Worldと、ラテンアクションベースの手法GO-1（ViLLAフレームワーク）を用いて、多領域の二腕・触覚操作を単一のポリシーで学習。
ラテンプランナーとディフュージョンアクションエキスパートを組み合わせることで、RDT-1Bと比較して平均32％の性能向上を実現し、データスケーリング法則と品質の重要性を実証。"
"3D, End-to-End, Hierarchical",SAM2Act,SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation,ICML,"Allen Institute for AI, NVIDIA, University of Washington",https://arxiv.org/abs/2501.18564,https://sam2act.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Simulation","LoRA, Supervised","Colosseum, RLBench","Depth, Language, PointCloud, Vision","Colosseum, RLBench","CLIP, SAM2",Heatmap,"SAM2Actは、SAM2を基盤とした多解像度アップサンプリング・マルチビュートランスフォーマーで、高精度かつ汎用的な3D操作を学習し、RLBenchで86.8%、干渉耐性で△4.3%を達成しました。
SAM2Act+はメモリバンクとアテンションを追加し、MemoryBenchの空間記憶タスクで94.3％の成功率を達成し、実ロボットでもSOTAを更新しました。"
"3D, Hierarchical, Planning",LMM-3DP,Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation,,UC San Diego,https://arxiv.org/abs/2501.18733,https://lmm-3dp-release.github.io/,Manipulation,Kitchen,Franka Emika Panda,Supervised,,"Language, PointCloud, Proprioception, Vision",,"CLIP, GPT, PointNext",Keyframe,"LMM-3DPは、GPT-4Vを基盤とした視覚ループ計画＋Critic/MemoryとDINO-PointNext-3D Transformer言語条件3Dポリシー（手法）・スキルライブラリを統合し、厨房の長距離作業の成功率を1.5倍以上向上させました。
視覚フィードバック・自己批判・few-shot 3Dスキル学習により、動的干渉・失敗の再試行・新技術習得までを自律的に処理します。"
"End-to-End, Open-Source",SmolVLA,SmolVLA: A vision-language-action model for affordable and efficient robotics,,Hugging Face,https://arxiv.org/abs/2506.01844,https://huggingface.co/papers/2506.01844,Manipulation,Tabletop,"SO-100, Simulation",Supervised,"LIBERO, MetaWorld","Language, Proprioception, Vision","LIBERO, MetaWorld","SigLIP, SmolLM2",Flow Matching,小型VLM + Flow-Matching Expertにレイヤースキップ・トークン削減・クロスCA/SA・非同期推論手法を適用し、GPU 1枚で学習・CPU実行が可能。コミュニティデータ<30kエピソードで事前学習し、LIBERO・Meta-World・シロボットで大規模VLA並みの性能を達成。
End-to-End,GRAPE,GRAPE: Generalizing Robot Policy via Preference Alignment,,University of Washington,https://arxiv.org/abs/2411.19309,https://grape-vla.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Simulation","RL, Supervised","LIBERO, SIMPLER","Language, Vision",,"DINOv2, GPT, OpenVLA, SAM","Autoregressive, Discrete Token",GRAPEは、段階別のキーポイントコストとTPOの好みを基に学習し、失敗まで含めて学習することで、VLAポリシーを目標指向的に一般化します。その結果、ドメイン内51.8％、OOD58.2％の成功率向上・衝突37.4％減少を達成しました。
End-to-End,TRA,Temporal Representation Alignment: Successor Features Enable Emergent Compositionality in Robot Instruction Following,,"Cornell University, UC Berkeley",https://arxiv.org/abs/2502.05454,https://tra-paper.github.io/,Manipulation,Tabletop,"Simulation, WidowX","Self-Supervised, Supervised",OGBench,"Language, Vision","BridgeV2, OGBench",,Continuous,TRAは、現在の状態・将来の目標・言語をInfoNCEで時間同期させ、同一の表現空間に統合し、単純なBCのみでも段階的な計画なしに複合タスクを合成します。実ロボット13タスクとOGBench7環境において、GCBC・AWRと比較して最大40％以上の性能向上を示し、整列損失のみでも構成的汎化が可能であることを証明しています。
End-to-End,RoboBERT,RoboBERT: An End-to-end Multimodal Robotic Manipulation Model,,,https://arxiv.org/abs/2502.07837,https://anonymeskonto.github.io/Web/,Manipulation,Tabletop,"RM65, Simulation",Supervised,CALVIN,"Language, Vision",CALVIN,"BERT, CLIP",Diffusion,"RoboBERTは、BERT（言語）とCLIP（ビジョン）を融合させた後、CNN拡散ポリシーを2段階学習（1段階目：ポリシー、2段階目：自然言語）し、頑強な拡張を適用することで、CALVINとシロボットの両方でSOTAを達成した208MパラメーターのエンドツーエンドVLAモデルです。
2段階戦略は、少ないロボットデータでも言語の多様性を吸収しつつ、ポリシー性能の低下なしに高い成功率を保証します。"
End-to-End,Dita,Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy,,"Peking University, Tsinghua University, Zhejiang University",https://arxiv.org/abs/2410.15959v4,https://robodita.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,"CALVIN, LIBERO, Maniskill2, SIMPLER","Language, Vision",Open-X-Embodiment,"CLIP, DINOv2, Transformer",Diffusion,Diffusion Transformer Policyは、言語・視覚トークンとノイズが混ざった7次元アクションチャンクを単一の大型Transformerで確率拡散デノイジングし、連続制御を学習します。大規模なX-Embodiment事前学習後、CALVIN、SimplerEnv、LIBERO、Maniskill2、実際のFranka Armにおいて、Octo・OpenVLAを超える汎化性能を達成しました。
"3D, Hierarchical, Planning",SoFar,SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation,,"Peking University, Shanghai Jiao Tong University, Tsinghua University",https://arxiv.org/abs/2502.13143,https://qizekun.github.io/sofar/,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,"Open6DOR, SIMPLER, SpatialBench","Depth, Language, PointCloud, Vision","Open6DOR, OrienText300K, SpatialBench","CLIP, Florence-2, GPT, PointNet, SAM",Primitive,"Semantic Orientationと35万サンプルのOrienText300Kで学習した3D言語トランスフォーマーPointSOをSAM・Florence-2・VLMと組み合わせたSOFARは、物体の位置と方向を同時に理解し、6自由度ロボットの計画・操作までゼロショットで実行します。
シミュレーション（Open6DOR V2で48.7%向上）・SIMPLER・現実世界での60のタスクにおいてRT・VLAモデルを凌駕し、方向VQA・指向ナビゲーションなどへの適用範囲を拡大します。"
"3D, End-to-End, Prediction",ARM4R,Pre-training Auto-regressive Robotic Models with 4D Representations,ICML,"BAIR, UC Berkeley",https://arxiv.org/abs/2502.13142,https://arm4r.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Kinova Gen3, Simulation",Supervised,RLBench,"Language, PointCloud, Proprioception, Vision",EPIC-Kitchens,"CLIP, ViT","Autoregressive, Continuous","人間映像から3Dポイントトラックを予測するようにオートリグレッシブ・トランスフォーマーを事前学習し、これをロボットの状態予測に置き換え・微調整して、低次元4D表現をロボット制御に転移する。
4D事前学習のみにより、RLBench・実験ロボットにおいて、従来の2D/VLA・3Dボクスル方式よりも高い成功率を達成。"
Hierarchical,Atomic Skill,An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation,,China,https://arxiv.org/abs/2501.15068,,"Bi-manual, Manipulation",Tabletop,ALOHA,,,"Language, Vision",,"GPT, Octo, RDT1B, SAM2",,"3-Wheel フレームワーク（VLP 分解 → 原子スキル定義 → VLA 少例学習チューニング）により、スキルライブラリを自己拡張し、データ要件を大幅に削減しつつ、エンドツーエンドよりも高い成功率と汎用性を実現。
つまり、「作業を分割し、スキルを微調整して累積・再利用する」手法により、データ爆発なしに現実世界のロボット操作を拡張できる。"
Hierarchical,VLAS,VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation,ICLR,Zhejiang University,https://arxiv.org/abs/2502.13508,,Manipulation,Tabletop,"Simulation, UR5",Supervised,CALVIN,"Audio, Language, Vision",,"CLIP, Vicuna, Whisper","Autoregressive, Discrete Token","音声入力を含むエンドツーエンドのVLAモデル（VLAS）は、Whisper-CLIP-Vicunaを組み合わせ、Voice RAGで個人知識を注入し、音声・動画・テキストから行動トークンを直接生成します。
SQA・CSIデータと3段階（音声-テキスト照合→音声QA→行動BC）の学習により、CALVIN・カスタムタスクにおいて既存のVLA+ASRと比較して高い成功率を達成します。"
"Affordance, Prediction, VLM for Robotics",RoboBrain,RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete,CVPR,"Peking University, The University of Hong Kong",https://arxiv.org/abs/2502.21257,https://superrobobrain.github.io/,Manipulation,"Kitchen, Tabletop",Simulation,"LoRA, Supervised","AGD20K, OpenEQA, RoboVQA, ShareRobot QA","Language, Vision","Open-X-Embodiment, ShareRobot","Qwen2.5-VL, SigLIP","Autoregressive, Discrete Token",ShareRobot(102万QA-アフォーダンス-軌跡)で学習したLLaVAベースのRoboBrainは、Planning-A-LoRA-T-LoRAモジュールにより、長い作業計画、アフォーダンス、軌跡を一度に生成する。この統合手法により、RoboVQA-OpenEQAなどでGPT-4Vや最新のオープンソースモデルを上回るSOTA性能を達成しました。
End-to-End,iManip,iManip: Skill-Incremental Learning for Robotic Manipulation,,China,https://arxiv.org/abs/2503.07087,,Manipulation,Tabletop,"Franka Emika Panda, Simulation","Distillation, Supervised",RLBench,"Depth, Language, PointCloud, Proprioception, Vision",RLBench,CLIP,Continuous,iManipは、**タイムバランスサンプリング(TRS)と可変PerceiverIO(E-PIO)**を組み合わせて、ロボットがスキルを順次増やしても忘却することなく素早く適応できるようにしたSILフレームワークです。RLBench-実世界評価で、従来のリハーサル・知識蒸留法よりも平均成功率が9%以上向上し、軽量・拡張性を実証しました。により、新しい課題・新しい機構まで素早く適応・性能向上する。
"End-to-End, Prediction, RL",LUMOS,LUMOS: Language-Conditioned Imitation Learning with World Models,,"University of Freiburg, University of Oxford, University of Technology Nuremberg",https://arxiv.org/abs/2503.10370,http://lumos.cs.uni-freiburg.de/,Manipulation,Tabletop,"Franka Emika Panda, Simulation","RL, Self-Supervised",CALVIN,"Language, Proprioception, Vision",CALVIN,"Dreamer, Sentence-BERT",Continuous,世界モデルの潜在空間からLatent Plan CVAE + actor-critic + DITTO intrinsic rewardで言語条件の長期操作をオフライン学習し、実ロボットにゼロショット転移する。CALVINと実際の14タスクで従来のHULC-GCBCなどを上回り、covariate shiftを緩和する。
End-to-End,PPL,"Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation",CVPR,"Shanghai Artificial Intelligence Laboratory, Shanghai Jiao Tong University, Zhejiang University",https://arxiv.org/abs/2504.00420,,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,"LIBERO, MimicGen","Language, Proprioception, Vision","LIBERO, MimicGen","CLIP, RAFT",Diffusion,"Primitive Prompt Learning： モーション+テキストクエリでプリミティブプロンプトを学習し、新しいスキルにはlifelongプロンプトだけを追加して、経験リプレイがなくてもFWT/BWT SOTA。
コア手法 = Motion-Aware Prompting (Optical Flow + CLIP) + Diffusion-Transformer prefix-prompt insertでパラメータ効率的なライフロン遷移。"
"End-to-End, Hierarchical",SPECI,SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation,,China,https://arxiv.org/abs/2504.15561,,Manipulation,,Simulation,Supervised,LIBERO,"Language, Proprioception, Vision",LIBERO,CLIP,Continuous,SPECIは**「動的スキルコードブック+ Mode Approximation」**でマルチモーダル認知・スキル・行動をend-to-end階層化し、CILで転移・忘却を同時に解決する。実験結果、4つのLIBEROスイート全体でFWT↑-NBT↓-AUC↑で、従来のBUDS-LOTUSなどを大きく上回った。
End-to-End,PPI,Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation,RSS,"Peking University, Shanghai Artificial Intelligence Laboratory, Zhejiang University",https://arxiv.org/abs/2504.17784,https://yuyinyang3y.github.io/PPI/,"Bi-manual, Manipulation",Tabletop,"Franka Emika Panda, Simulation",Supervised,RLBench2,"Depth, Language, PointCloud, Proprioception, Vision",RLBench2,"CLIP, DINOv2, PointNet++",Diffusion,PPIは、キーフレームGripper Keyposeと3D Object Pointflowを条件として、連続動作を確率的拡散・変換器で生成するエンドツーエンドの両腕操作方針である。RLBench2 80.8%・実世界4課題SR 62.5%で、従来のキーフレーム・連続制御法を上回るSOTA性能を達成した。
"Hierarchical, Prediction, VLM for Robotics",RoboGround,RoboGround: Robotic Manipulation with Grounded Vision-Language Priors,CVPR,"Shanghai Artificial Intelligence Laboratory, Zhejiang University",https://arxiv.org/abs/2504.21530,https://robo-ground.github.io/,Manipulation,Tabletop,Simulation,Supervised,RoboCasa,"Language, Proprioception, Vision",RoboCasa,"CLIP, GLaMM, GR-1","Autoregressive, Continuous",Grounded VLMが予測したターゲット/配置マスクをTransformerポリシーに注入し、自動生成した大規模な複合シミュレーションデータ(24 k/112 k)で学習し、複雑・画像環境で従来のGR-1に比べて2倍以上の成功率を達成したマスクベースのロボット操作手法。
Prediction,GR-MG,GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal-Conditioned Policy,RAL,"ByteDance Research, China",https://arxiv.org/abs/2408.14368,https://gr-mg.github.io/,Manipulation,Tabletop,"Kinova Gen3, Simulation","Self-Supervised, Supervised",CALVIN,"Language, Proprioception, Vision","CALVIN, Ego4D, RT-1, Something-Something",T5,"Autoregressive, Continuous",部分注釈(言語のみ・行動のみ)データを併用する「進行度・ガイド目標画像+言語」マルチモーダルポリシーGR-MGを提案、拡散ベースの目標生成器とGPT-styleポリシーで学習。 シミュ‧実ロボットベンチマークで5-連続成功率23pt↑および58課題78％成功達成、few-shot新規スキルも従来比15pt↑。
End-to-End,pi0-TLI,Task Reconstruction and Extrapolation for π_0 using Text Latent,,,https://arxiv.org/abs/2505.03500,,Manipulation,Tabletop,Simulation,,LIBERO,"Language, Proprioception, Vision",LIBERO,Pi0,Flow Matching,"VLA(π₀)から各タスクのテキストlatent(トークンヒドゥンステート平均)を抽出し、extrapolated taskに対して2つのlatentを**時間的に補間(TLI)**してヒドゥンステートに追加することで、既存のタスクのプリミティブを組み合わせて微小なextrapolated taskも高い成功率(83%)で解決。
既存のVLAは、オブジェクトの意味理解の代わりに位置に過度にオーバーフィッティングする傾向があり、latent操作はプライベートインストラクション/バックドア攻撃にも活用される可能性がある。"
End-to-End,Real Time Chunking,Real-Time Action Chunking with Large Models,,"Physical Intelligence, UC Berkeley",https://arxiv.org/abs/2506.07339,https://www.pi.website/research/real_time_chunking,"Bi-manual, Manipulation",Tabletop,ALOHA,Supervised,Kinetix,"Proprioception, Vision",,Pi0.5,Flow Matching,**リアルタイムチャンキング(RTC)**は、diffusion/flowベースのVLAポリシーにinference段階で適用する非同期チャンキングおよびinpainting手法で、遅延のある環境でもスムーズで高速なロボット制御を実現します。すでに実行されたアクションを「固定」し、残りはソフトマスキングベースのインペインティングで生成することで、再学習なしで連続的な動作と高い成功率を実現します。
"End-to-End, Hierarchical, Open-Source",OpenHelix,"OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation",,"HKUST, Zhejiang University",https://arxiv.org/abs/2505.03912,https://openhelix-robot.github.io/,Manipulation,Tabletop,Simulation,Supervised,CALVIN,"Depth, Language, Proprioception, Vision",,LLaVA,Diffusion,"デュアルシステムVLA構造の主要な設計要素と既存の方法を分析し、プロンプトチューニングと補助タスクを適用した軽量オープンソースのデュアルシステムVLA(OpenHelix)を提案し、実験的に動的/言語一般化環境で優れた性能を示しました。
手法：大型MLLMはプロンプトチューニング+補助タスクで、ポリシーは事前学習されたディフュージョンベースポリシーをファインチューニングし、2つのシステムの接続はMLPプロジェクターの事前整列を行う。"
End-to-End,CrayonRobo,CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation,CVPR,Peking University,https://arxiv.org/abs/2505.02166,,Manipulation,Tabletop,"Franka Emika Panda, Simulation","LoRA, Supervised",SAPIEN,"Depth, Language, Vision",PartNet-Mobility,"CLIP, LLaMA",Discrete Token,2Dビジュアルプロンプトで各キーフレームの目標（接触点、方向など）を明確に入力し、ビジョン・言語・行動モデルが3D操作を予測する方式。実/シミュレーションともに、従来方式よりも複雑・長期・未知のタスクにも堅牢な性能を示す。
"End-to-End, Hierarchical",HiRT,HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers,CoRL,"Tsinghua University, UC Berkeley",https://arxiv.org/abs/2410.05273,,Manipulation,Tabletop,"Franka Emika Panda, Simulation","LoRA, Supervised","FrankaKitchen, MetaWorld","Language, Vision","FrankaKitchen, MetaWorld","InstructBLIP, LLaMA",Continuous,"HiRTは、遅いVLMで長期的な情報を抽出し、軽量政策がリアルタイム制御を行う階層的政策を提案し、従来のVLAモデルの遅い速度を克服し、動的タスク性能(成功率/速度)を大幅に向上させた。
手法：非同期階層政策、VLMベースのlatent-conditioning、リアルタイム軽量政策。"
"End-to-End, Hierarchical",UniVLA,UniVLA: Learning to Act Anywhere with Task-centric Latent Actions,RSS,The University of Hong Kong,https://arxiv.org/abs/2505.06111,https://github.com/OpenDriveLab/UniVLA,"Manipulation, Navigation",Tabletop,"AgileX, Franka Emika Panda, Simulation, WidowX","Self-Supervised, Supervised","CALVIN, Habitat, LIBERO, SIMPLER","Language, Vision","BridgeV2, Ego4D, GNM, Open-X-Embodiment","DINOv2, LLaMA2, SigLIP, T5","Autoregressive, Discrete Token",タスク中心のlatent actionを非教師付き学習し、様々なロボット/人映像からアクションラベルなしで汎用VLAポリシーを学習するUniVLAを提案、小型デコーダで各実施形態に効率的に適用可能。OpenVLAに比べてはるかに少ないデータ/演算でSOTA性能を達成。
"Cross-embodiment, Hierarchical, Prediction",LangToMo,Pixel Motion as Universal Representation for Robot Control,,Stony Brook University,https://arxiv.org/abs/2505.07817,https://kahnchana.github.io/LangToMo/,"Manipulation, Navigation","Indoor, Tabletop","Simulation, xArm","Self-Supervised, Supervised","AI2THOR, MetaWorld","Language, Vision","MetaWorld, Open-X-Embodiment","Universal Sentence Encoder, ViT",Diffusion,ピクセルモーション（オプティカルフロー）の予測を、言語ベースのロボット制御の汎用的な中間表現として使用し、ディフュージョンモデルを用いて自己教師付きの大規模なビデオ・キャプション学習を実施。予測されたピクセルモーションをタスクごとのアクションベクトルに変換し、ゼロショットおよび多様なロボット/環境に対して一般化された制御を実現する。
"Hierarchical, Planning",CLIP-RT,CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision,RSS,Seoul National University,https://arxiv.org/abs/2411.00508,https://clip-rt.github.io/,Manipulation,Tabletop,"Simulation, UR5",Self-Supervised,LIBERO,"Language, Vision",Open-X-Embodiment,CLIP,Primitive,自然言語の指示のみを用いてロボットデータを収集・拡張するフレームワークと、自然言語ベースのモーションプリミティブを対照的模倣学習で学習するビジョン・言語・行動（VLA）モデルCLIP-RTを提案する。実際のUR5およびシミュレーションにおいて、SOTA（OpenVLA）と比較して24％高い成功率と7倍少ないパラメーターで、優れた一般化・適応・協業能力を示した。
End-to-End,ECoT-Lite,Training Strategies for Efficient Embodied Reasoning,,"Physical Intelligence, Stanford University, UC Berkeley",https://arxiv.org/abs/2505.08243,https://ecot-lite.github.io/,Manipulation,Tabletop,"Simulation, WidowX",Supervised,"BridgeV2, LIBERO","Language, Proprioception, Vision","BridgeV2, LIBERO",MiniVLA,"Autoregressive, Discrete Token","ECoT-Liteは、CoT推論を必要とせずに推論ベースのポリシーの汎化性能をほぼ維持しつつ、推論事前学習やドロップアウトなどのシンプルな学習手法のみでリアルタイム性も確保したロボットポリシー学習手法です。
主要な手法：推論事前学習、推論ドロップアウトによる表現強化と推論の高速化
"
"Affordance, Hierarchical, Prediction",A_0,A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation,,China,https://arxiv.org/abs/2504.12636,https://a-embodied.github.io/A0/,Manipulation,Tabletop,"Dobot, Franka Emika Panda, Kinova Gen3, Realman","Self-Supervised, Supervised",,"Depth, Language, Vision","DROID, HOI4D, ManiSkill, PixMo-One-Point","Qwen2.5-VL, SigLIP",Diffusion,"ロボット操作のための階層的アフォーダンス認知拡散モデル（A0）を提案し、オブジェクト中心の接触点と軌跡予測を通じて、多様なプラットフォームに汎用可能なエンボディメント非依存型アフォーダンス表現を学習する。
100万件の接触点データによる事前学習、位置オフセットアテンションなどにより、複雑な実際のタスクにおいて最先端（SOTA）の性能を達成し、ロボット/タスクの汎用性・効率性ともに優れている。"
"End-to-End, Tactile",VTLA,VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation,,China,https://arxiv.org/abs/2505.09577,https://sites.google.com/view/vtla,"Manipulation, Peg-in-Hole","Assembly, Tabletop",UR3,"DPO, Supervised",,"Language, Tactile, Vision",VTLA,Qwen2-VL,Autoregressive,"ビジョン-触覚-言語統合政策モデル（VTLA）を提案し、シミュレーションベースの学習とDPOベースのプレファレンス学習を通じて、実際の接触組み立て（ペグ・イン・ホール）において95％以上のsim2real成功率を達成しました。
主要な手法：ビジョンガイド型時系列トークン、DPOベースのプレファレンス学習、マルチモーダルデータドメインランダム化。"
"End-to-End, Hierarchical",OneTwoVLA,OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning,,"Shanghai Artificial Intelligence Laboratory, Tsinghua University",https://arxiv.org/abs/2505.11917,https://one-two-vla.github.io/,"Bi-manual, Manipulation","Kitchen, Tabletop","AgileX, Franka Emika Panda",Supervised,,"Language, Proprioception, Vision",,,,"OneTwoVLAは、推論と行動を適応的に切り替える統合型VLAモデルであり、手動による推論アノテーションと大規模な合成ビジョン・言語データによるコトレーニングを通じて、長期計画、エラー回復、相互作用、一般化において優れた性能を発揮します。
従来のデュアルシステムモデルの限界を克服し、推論と行動を両立させる適応型フレームワークと合成エンボディッド推論データパイプラインを提案した点が、本手法の核心です。"
End-to-End,InSpire,InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasonin,,China,https://arxiv.org/abs/2505.13888,https://koorye.github.io/proj/Inspire/,Manipulation,Tabletop,"AgileX, Simulation",Supervised,LIBERO,"Language, Vision",LIBERO,"MiniVLA, Pi0, Qwen2.5-VL","Autoregressive, Discrete Token","InSpireは、「ロボットと対象物の方向」に関する質問を挿入することで、VLAが本質的な空間関係に注目するようにし、追加のデータなしで既存のVLAの一般化性能を効果的に向上させるプラグイン方式の手法です。
シミュレーションとリアルロボット実験の両方で、SOTA（最先端技術）と比較して明確な成功率の向上効果を示しています。"
"End-to-End, Tactile",ForceVLA,ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation,,"National University of Singapore, Shanghai Jiao Tong University, Shanghai University",https://arxiv.org/abs/2505.22159,https://sites.google.com/view/forcevla2025,"Manipulation, Peg-in-Hole","Assembly, Tabletop",Flexiv Rizon,Supervised,,"Language, Proprioception, Tactile, Vision",ForceVLA,"PaliGemma, SigLIP",Flow Matching,"力センサー信号をモダリティとして統合するMixture-of-Experts（MoE）ベースのForceVLAを提案し、視覚・言語・力融合ポリシーをエンドツーエンドで学習することで、接触が豊富な操作において既存手法比で平均23.2%向上した性能を達成しました。
手法：π0 VLAアーキテクチャを基盤に、FVLMoE（力感知型MoE）による後段融合（late fusion）およびリアルタイム力フィードバックルーティングを実施。Flexiv Rizon 7-DOFロボット（適応型グリッパー＋力センサー）を用いて、5つの実際の操作タスク実験を実施しました。"
"End-to-End, Hierarchical, Humanoid",Hume,Hume: Introducing System-2 Thinking in Visual-Language-Action Model,,"Shanghai Artificial Intelligence Laboratory, Shanghai Jiao Tong University, Zhejiang University",https://arxiv.org/abs/2505.21432,https://hume-vla.github.io/,"Bi-manual, Manipulation","Kitchen, Tabletop","AgiBot G1, Franka Emika Panda, Simulation, WidowX","RL, Supervised","LIBERO, SIMPLER","Language, Proprioception, Vision",Open-X-Embodiment,"DINOv2, PaLI",Flow Matching,"価値に基づくアクション候補サンプリングと連続デノイジング（cascaded denoising）を組み合わせたデュアルシステムVLAモデルにより、遅い思考（System-2）と速い反応（System-1）を非同期的に結合し、複雑なロボット操作においてSOTAを達成。
主要な手法：Value-guided Best-of-Nサンプリング + 非同期的cascadedアクションデノイジング。"
"Hierarchical, Planning",Agentic Robot,Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents,,"Harvard University, MIT, Shanghai Jiao Tong University",https://arxiv.org/abs/2505.23450,https://agentic-robot.github.io/,Manipulation,,Simulation,"LoRA, Supervised",LIBERO,"Language, Vision",LIBERO,"GPT, OpenVLA, Qwen2.5-VL",Autoregressive,SAPという標準化されたプロトコルを用いて、プランナー（大規模LLMベースのサブゴール分解）、実行機（VLA）、検証機（VLM）を構造化し、サブゴールごとの進行・検証・復旧を反復する閉ループ脳型ロボットシステムを提案する。ロング・ホライズン操作におけるエラーの蓄積防止とリアルタイム復旧により、SOTA性能を達成する。
End-to-End,TrackVLA,TrackVLA: Embodied Visual Tracking in the Wild,,Peking University,https://arxiv.org/abs/2505.23189,https://pku-epic.github.io/TrackVLA-web/,Navigation,"Indoor, Outdoor","Simulation, Unitree Go1",Supervised,"EVT-Bench, Gym-UnrealCV, Habitat","Language, Vision","ActivityNet, EVT-Bench, MovieChat, PANDA, SYNTH-PEDES","EVA-CLIP, Vicuna",Diffusion,"TrackVLAは、ビジョン・言語・行動（VLA）モデルであり、大規模な認識およびロボット追跡データを活用してターゲット認識と経路計画を統合学習し、現実世界の動的環境においてゼロショット頑健な視覚追跡を実現しました。

言語モデリング（ヘッド）とアンカーベースの拡散軌道計画（ヘッド）を組み合わせ、170万サンプルの共同学習とEVT-Benchベンチマークにおいて最高性能を示しています。"
End-to-End,SwitchVLA,SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models,,China,https://arxiv.org/abs/2506.03574,https://switchvla.github.io/,"Bi-manual, Manipulation",Tabletop,"Franka Emika Panda, Simulation",Supervised,LIBERO,"Language, Proprioception, Vision",LIBERO,"BART, DaViT, Florence-2",Flow Matching,"SwitchVLAは、ビジョン・言語・行動ロボットがリアルタイムのコマンド変更に対応し、接触状態とコマンド履歴を利用してForward/Advance/Rollback行動を条件付きで生成するポリシーを学習する実行認識型タスクスイッチングフレームワークです。
別途スイッチデータなしで既存のデモのみで学習し、既存のVLAと比較して実機およびシミュレーションの両方で滑らかで頑強なタスク切り替え性能を示します。"
"3D, End-to-End",OG-VLA,OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation,,"Georgia Tech, NVIDIA, University of Southern California",https://arxiv.org/abs/2506.01196,https://og-vla.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,"ARNOLD, Colosseum","Depth, Language, PointCloud, Vision","ARNOLD, Colosseum","ImageBind, StableDiffusion, Vicuna, X-VILA",Image Generation,"OG-VLAは、マルチビューRGBD入力を3Dポイントクラウドに統合し、正射影画像に変換した後、LLMと画像拡散モデルを用いてキーフレームアクションをヒートマップ形式で予測する3D対応型VLAポリシーです。
この手法は、少ないデータでも従来の方法に比べて未知のコマンド/シーン/物体の一般化と精密な3D操作において優れた性能を発揮します。"
End-to-End,LoHoVLA,LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks,,Shanghai Jiao Tong University,https://arxiv.org/abs/2506.00411,,Manipulation,Tabletop,Simulation,Supervised,RAVENS,"Depth, Language, Vision",LoHoSet,"PaliGemma, SigLIP","Autoregressive, Discrete Token","LoHoVLAは、高水準のサブタスク計画と低水準のアクション予測を単一のVLAモデル（PaliGemmaベース）に統合し、階層的クローズドループ制御を適用することで、長期タスクにおいて従来の手法に比べてより優れた性能と汎化性を実現しました。
（手法：大規模VLMベースの言語・アクション同時生成、失敗時の自動再計画、シミュレーターによる大規模データセットの構築）"
"End-to-End, Prediction",Unified Video Action Model,Unified Video Action Model,RSS,Stanford University,https://arxiv.org/abs/2503.00200,https://unified-video-action-model.github.io/,Manipulation,Tabletop,"AgileX, Simulation","Self-Supervised, Supervised",LIBERO,"Language, Vision",UMI,CLIP,Diffusion,ビデオとアクションを統合して潜在的に学習し、推論時にアクションのみを高速に生成するデカップルド拡散構造およびマスキングベースの多機能学習方式を提案する。ロボットポリシー、ビデオ予測、ダイナミクスモデルなど、多様なロボット応用において単一のモデルで最先端（SOTA）性能を達成する。
"End-to-End, RL",VLA-RL,VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning,,"Nanyang Technological University, Tsinghua University",https://arxiv.org/abs/2505.18719,https://github.com/GuanxingLu/vlarl,Manipulation,Tabletop,Simulation,RL,LIBERO,"Language, Vision",LIBERO,"DINOv2, LLaMA2, OpenVLA, SigLIP","Autoregressive, Discrete Token","VLA-RLは、事前学習されたVLA（例：OpenVLA-7B）にオンライン強化学習とロボットの過程報酬モデル（reward model）、カリキュラム学習などの実装手法を適用し、多様なロボット操作タスクの汎化性能を大幅に向上させたフレームワークです。
LIBEROベンチマークにおいて4.5%の性能向上を実現し、既存の商用モデルと類似した性能を達成しました。"
"End-to-End, Training",BitVLA,BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation,,China,https://arxiv.org/abs/2506.07530,https://github.com/ustcwhy/BitVLA,Manipulation,Tabletop,Simulation,"Distillation, Supervised",LIBERO,"Language, Proprioception, Vision","LIBERO, LLaVA-LCS, MammoTH-VL","BitNet, SigLIP",Parallel Decoding,BitVLAは、すべてのパラメーターを1ビット（3値）に制限した最初のVLAモデルであり、ディスティレーションを意識した量子化（教師-生徒ベースの潜在的アラインメント）手法を用いてビジョンエンコーダーを極限まで圧縮し、SOTAレベルのロボット操作性能とメモリ使用量を3分の1以下に同時に達成しました。
End-to-End,Fast ECoT,Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse,,"University College London, University of Freiburg",https://arxiv.org/abs/2506.07639,,Manipulation,Tabletop,"Franka Emika Panda, Simulation","LoRA, Supervised",LIBERO,"Language, Proprioception, Vision","BridgeV2, DROID, LIBERO, Open-X-Embodiment","ECoT, OpenVLA",Autoregressive,"ECoTの反復的な推論をキャッシュ/再利用し、推論の並列化およびアクション-推論の非同期スケジューリングを導入することで、追加の学習なしで最大7.5倍高速なリアルタイムVLAロボット制御を実現しました。

従来の方法とは異なり、推論を段階ごとに独立して並列に生成し、キャッシュを活用して不要な計算を削減した点が核心的な手法です。"
"3D, End-to-End",BridgeVLA,BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models,,China,https://arxiv.org/abs/2506.07961,https://bridgevla.github.io/home_page.html,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,"Colosseum, GemBench, RLBench","Depth, Language, PointCloud, Vision","Colosseum, GemBench, RLBench, RoboPoint","PaliGemma, SigLIP",Heatmap,3Dポイントクラウドを複数の2D画像に投影し、VLMと入出力データを2Dヒートマップ空間で整列させ、大規模な2Dオブジェクト検出事前学習（ヒートマップ事前学習）を通じて3D操作ポリシーを効率的に学習。RLBenchなど多様なベンチマークと実際のロボットにおいて、SOTA（最先端）および強い汎化性能を達成。
"End-to-End, Failure-Recovery",SAFE,"SAFE: Multitask Failure Detection for Vision-Language-Action Models ",,Toyota Research Institute,https://arxiv.org/abs/2506.09937,https://vla-safe.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,"LIBERO, SIMPLER","Language, Proprioception, Vision","BridgeV2, DROID, LIBERO","OpenVLA, Pi0",,VLA内部のフィーチャーの構造的特性を活用し、多様なタスクにおける失敗を早期に検出できるマルチタスク失敗検出器（SAFE）を提案した。SAFEはMLP/LSTMベースのフィーチャープロービングとコンフォーマル予測により、既存のシステムに比べて未知のタスクにおいてより正確で高速な失敗検出性能を実現する。
Evaluation,INT-ACT,From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models,,New York University,https://arxiv.org/abs/2506.09930,https://ai4ce.github.io/INT-ACT/,Manipulation,Tabletop,Simulation,Supervised,INT-ACT,"Language, Vision",BridgeV2,"Magma, Octo, Pi0, SpatialVLA",,"シミュレーションベースの50個のタスクを用いて、INT-ACTスイートで最新のVLAモデルの一般化限界を体系的に評価しました。その結果、意図は適切に理解できるものの、行動の実行で失敗する「Intention-Action Gap」と、言語/ビジョン変換における一般化脆弱性を確認しました。
手法：SimplerEnvベースのシミュレーション、BridgeV2データセット、π0、SpatialVLA、Magma、Octoなど多様なモデルを用いた詳細なOOD評価および新たな「Intention Correct Rate」メトリクスの導入
."
End-to-End,TUDP,Time- Diffusion Policy with Action Discrimination for Robotic Manipulation,,China,https://arxiv.org/abs/2506.09422,,Manipulation,Tabletop,"Simulation, UR5",Supervised,RLBench,"Language, Proprioception, Vision",RLBench,CLIP,Diffusion,TUDPは、時間統合型速度場と行動判別ネットワークを組み合わせることで、反復回数を削減しつつも正確なロボット操作行動を迅速に生成します。RLBenchと実際の実験において、既存のSOTAを上回る成功率を示しており、action-wise trainingとtime-unified denoisingが核心的な手法です。
"End-to-End, Hierarchical",RationalVLA,RationalVLA: A Rational Vision-Language-Action Model with Dual System,,"HKUST, Shanghai Jiao Tong University",https://arxiv.org/abs/2506.10826,https://irpn-eai.github.io/RationalVLA/,Manipulation,Tabletop,"AgileX, Simulation","LoRA, Supervised",CALVIN,"Language, Proprioception, Vision","CALVIN, RAMA benchmark","CLIP, LLaVA",Diffusion,RationalVLAは、デュアルシステム構造を採用し、高次元MLLMと低次元ポリシーを潜在空間で結合することで、欠陥のあるコマンドを拒否し、複雑なコマンドを正確に実行するモデルです。RAMAベンチマークにおいて、既存のモデルと比較して成功率を14.5%向上させました。
"End-to-End, Hierarchical",Fast-in-Slow,Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning,,China,https://arxiv.org/abs/2506.01953,https://fast-in-slow.github.io/,"Bi-manual, Manipulation",Tabletop,"AgileX, AlphaBot, Simulation",Supervised,RLBench,"Language, PointCloud, Proprioception, Vision","DROID, Open-X-Embodiment, ROBOMIND","DINOv2, LLaMA2, SigLIP",Diffusion,VLM内に高速行動モジュールを統合し、異種入力を用いた非同期デュアルシステム構造を提案するモデル。DiffusionとAutoregressiveを基盤とした共同訓練により、高い性能と高速処理を同時に実現したモデル。
End-to-End,OE-VLA,Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions,,Zhejiang University,https://arxiv.org/abs/2505.11214v1,,Manipulation,Tabletop,Simulation,Supervised,CALVIN,"Language, Vision",CALVIN,"Qwen1.5, SigLIP",,マルチモーダル入力（画像、動画など）を処理できるように、2段階のカリキュラム学習を活用したエンドツーエンドのVLAモデル。CALVINデータセットを用いてロボット操作実験を実施。
End-to-End,CEED-VLA,CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding,,"HKUST, Zhejiang University",https://www.arxiv.org/abs/2506.13725,https://irpn-eai.github.io/CEED-VLA/,"Bi-manual, Manipulation",Tabletop,"AgileX, Simulation","Distillation, Supervised","CALVIN, LIBERO","Language, Proprioception, Vision",,"DINOv2, LLaVA, OpenVLA, SigLIP","Discrete Token, Parallel Decoding",Jacobi ディコーディングと一貫性蒸留学習、混合ラベル指導、早期脱出ディコーディングを適用し、VLA モデルの推論を最大4倍高速化したモデルで、実際のロボットおよびシミュレーショントスクで検証済みです。
"Augmentation, End-to-End, Humanoid, Prediction",DreamGen,DreamGen: Unlocking Genearlization in Robot Learning through Video World Models,,"Caltech, KAIST, NVIDIA, UC Los Angeles, UC San Diego, University of Texas at Austin, University of Washington",https://arxiv.org/abs/2505.12705,https://github.com/NVIDIA/GR00T-Dreams,"Bi-manual, Dexterous Hand, Manipulation",Tabletop,"Franka Emika Panda, GR-1, SO-100, Simulation","LoRA, Self-Supervised, Supervised","DreamGen Bench, RoboCasa","Language, Vision","AgiBot World, BridgeV2, DROID, DexMimicGen, Ego4D, Language Table, RT-1, RoboCasa, Something-Something","CogVideoX, Cosmos, Hunyuan, SigLIP2, VQ-VAE, WAN2.1",Flow Matching,合成ビデオからIDMやLAPAを用いて行動を抽出し、ニューラル・トレジェクトリを構成し、これにより多様な行動と環境に一般化可能なポリシーを学習する。実際のロボットに対する最小限のテレオペレーションのみで、シミュレーションと現実世界において強力な一般化性能を達成する。
"RL, Training",RLRC,RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models,,Shanghai Jiao Tong University,https://arxiv.org/abs/2506.17639,https://rlrc-vla.github.io/,Manipulation,Tabletop,Simulation,"Quantization, RL, Supervised","LIBERO, Maniskill3","Language, Proprioception, Vision",,OpenVLA,"Autoregressive, Discrete Token",RLRCは、VLAモデルを構造的プルーニング、SFT+RLに基づく性能回復、4ビット量子化により、最大8倍のメモリ削減と性能の維持/向上を実現したVLA圧縮フレームワークです。WidowX-250SロボットアームでManiSkill3でテストされ、特にOOD一般化において優れた結果を示しました。
"End-to-End, Hierarchical, Prediction",MinD,MinD: Unified Visual Imagination and Control via Hierarchical World Model,,"HKUST, Peking University",https://arxiv.org/abs/2506.18897,https://manipulate-in-dream.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,RLBench,"Language, Proprioception, Vision",RT-1,"Dynamicrafter, OpenCLIP",Diffusion,MinDは、ビデオ生成器と行動ポリシーを組み合わせた階層的拡散ベースの世界モデルであり、リアルタイム操作と視覚的想像を統合し、映像と行動の一貫性を向上させました。DiffMatcherと共同学習戦略により、ビデオの特徴をポリシーに整合させ、RLBenchと実際のロボットにおいて最先端（SOTA）の性能を達成しました。
End-to-End,CronusVLA,CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation,,"Shanghai Artificial Intelligence Laboratory, Zhejiang University",https://arxiv.org/abs/2506.19816,https://lihaohn.github.io/CronusVLA.github.io/,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,"LIBERO, SIMPLER","Language, Vision","BridgeV2, Fractal, Open-X-Embodiment","DINOv2, LLaMA2, Qwen2.5, SigLIP","Autoregressive, Discrete Token","単一フレームVLAモデルを基盤に、モーション特徴量を活用したマルチフレームエンコーディングとクロスフレームデコーディングを組み合わせることで、効率的な行動予測を可能にするエンドツーエンドフレームワーク。
その後のファインチューニングにおいて、特徴量-行動リトリバルに基づく行動適応を導入し、精度と汎化能力を向上させる。"
"End-to-End, Prediction",WorldVLA,"WorldVLA: Towards Autoregressive Action World Model ",,China,https://arxiv.org/abs/2506.21539,https://github.com/alibaba-damo-academy/WorldVLA,Manipulation,Tabletop,Simulation,Supervised,LIBERO,"Language, Vision",LIBERO,"Chameleon, VQ-GAN","Autoregressive, Discrete Token","WorldVLAは、画像、テキスト、アクションを統合し、環境状態の予測とアクションの生成を同時に実行するオートリグレッションモデルです。世界モデルとポリシーモデルの相互補完的な学習を通じて、操作性能を向上させます。
アクションエラーの蓄積を防ぐアテンションマスキング戦略と混合学習により、長いシーケンスのアクションも安定して生成できるようにします。"
Special,ACTLLM,ACTLLM: Action Consistency Tuned Large Language Model,,University of Rochester,https://arxiv.org/abs/2506.21250,,Manipulation,Tabletop,Simulation,Supervised,"RAVENS, VIMA-Bench","Language, Vision",,LLaMA3,"Continuous, Discrete Token","視覚的観察を構造化されたシーン説明（JSON）に変換し、これに基づいて一貫した行動を推論するようにLLMをチューニングした操作モデルACTTLMを提案する。
Action consistency lossとmulti-turn visual dialogueフレームワークを導入することで、複雑なタスクにおいても一般化性能と文脈の一貫性を大幅に向上させた。"
"End-to-End, Policy",BAKU,BAKU: An Efficient Transformer for Multi-Task Policy Learning,NeurIPS,New York University,https://arxiv.org/abs/2406.07539,https://baku-robot.github.io/,"Manipulation, Whole-Body Control",Tabletop,"Simulation, xArm",Supervised,"DMC, LIBERO, MetaWorld","Language, Proprioception, Vision",,,Continuous,BAKUは、マルチモーダルエンコーダー、トランスフォーマーベースの観測トランク、およびチャンク化されたアクション予測を組み合わせたシンプルで効率的なトランスフォーマーアーキテクチャであり、少ないデモデータでも多様なタスクで強力な性能を達成しました。特に、LIBERO-90と実際のxArm操作タスクにおいて、既存のSOTA手法と比較して最大36%の性能向上を示しました。
"End-to-End, Policy",MDT,Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals,RSS,Karlsruhe Institute of Technology,https://arxiv.org/abs/2407.05996,https://intuitive-robots.github.io/mdt_policy/,Manipulation,Tabletop,"Franka Emika Panda, Simulation","Self-Supervised, Supervised","CALVIN, LIBERO","Language, Proprioception, Vision",,"CLIP, Voltron",Diffusion,MDTは、言語や画像など多様な目標モダリティから長期的な操作行動を学習する拡散ベースのトランスフォーマー政策です。MGFとCLAという自己教師付き補助損失を活用し、少数の言語注釈のみでも高い性能を達成します。
"3D, End-to-End",RoboMM,RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation,,China,https://arxiv.org/abs/2412.07215,https://robouniview.github.io/RoboMM.github.io/,Manipulation,Tabletop,Simulation,Supervised,"CALVIN, Colosseum, LIBERO, Maniskill2, MetaWorld, RLBench, RoboCAS, RoboCasa, Robomimic","Depth, Language, Proprioception, Vision",,"OpenFlamingo, UVFormer",Continuous,RoboMMは、OpenFlamingoを基盤としたマルチモーダルデコーダーとUVFormerを基盤とした3D認識アダプターを組み合わせた汎用操作ポリシーであり、**すべてのモダリティ（画像・行動・占有状態など）**を予測するエンドツーエンドアーキテクチャです。RoboDataは、9つのロボットデータセットを3D空間アラインメントとアクション正規化を通じて統合した大規模アラインメントデータセットです。
Planning,LLaRA,LLaRA: Supercharging Robot Learning Data for Vision-Language Policy,ICLR,Stony Brook University,https://arxiv.org/abs/2406.20095,https://github.com/LostXine/LLaRA,Manipulation,Tabletop,"Simulation, xArm","Self-Supervised, Supervised",VIMA-Bench,"Language, Vision",,LLaVA,Primitive,LLaRAは、既存のロボットデモデータを対話型インストラクションチューニングデータに変換し、画像座標に基づく自然言語アクション予測を通じてVLMをロボットポリシーに変換します。自己教師付き方式の補助データを活用することで性能を大幅に向上させ、小規模なデータでも強力な汎化能力を示します。
"3D, End-to-End",DI2,Depth Helps: Improving Pre-trained RGB-based Policy with Depth Information Injection,IROS,"China, Shanghai Artificial Intelligence Laboratory",https://arxiv.org/abs/2408.05107,https://gewu-lab.github.io/DepthHelps-IROS2024/,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Supervised,LIBERO,"Depth, Language, Vision",,OpenFlamingo,Continuous,DI²フレームワーク（Depth Completion ModuleとDepth-Aware Codebookを含む）を通じて、RGBのみの方針に深度情報に基づく空間知識を注入します。RGB-D学習とRGBのみ推論構造により、LIBEROおよび現実世界の操作タスクにおいて性能向上を実現しました。
"3D, End-to-End",RoboUniView,RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulation,,China,https://arxiv.org/abs/2406.18977,https://github.com/liufanfanlff/RoboUniview,Manipulation,Tabletop,Simulation,Supervised,CALVIN,"Depth, Language, Vision",,"CLIP, GPT-Neox, LLaMA, MPT, UVFormer","Autoregressive, Continuous","RoboUniViewは、複数の視点のイメージを3D占有率ベースで統合した統一されたビュー表現を事前学習し、これを基盤に言語と手首のイメージ情報を統合してロボットの操作行動を予測するモデルです。
UVFormerを用いて視覚情報と操作行動を分離して学習することで、視点の変化にも強い汎化性能を実現しました。"
"End-to-End, Prediction",GR-2,GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation,,ByteDance Research,https://arxiv.org/abs/2410.06158,https://gr2-manipulation.github.io/,Manipulation,"Industrial, Kitchen, Tabletop","Kinova Gen3, Simulation","Self-Supervised, Supervised",CALVIN,"Language, Proprioception, Vision","BridgeV2, EPIC-Kitchens, Ego4D, HowTo100M, Kinetics-700, RT-1, Something-Something","CLIP, VQ-GAN",Conditional VAE,"GR-2は、3,800万件のウェブ動画を用いて動画生成の事前学習を実施し、その成果を基に実際のロボットデータで微調整を行うことで、言語条件下で多様な操作をエンドツーエンドで実行するモデルです。
動画生成を基盤としたGPTトランスフォーマーアーキテクチャとcVAEを活用し、100種類以上の操作タスクとビンピッキングにおいて汎化能力を実証しました。"
"End-to-End, Prediction",Moto,Moto: Latent Motion Token as the Bridging Language for Robot Manipulation,ICCV,"The University of Hong Kong, UC Berkeley",https://arxiv.org/abs/2412.04445,https://chenyi99.github.io/moto/,Manipulation,Tabletop,"FANUC Mate, Simulation","Self-Supervised, Supervised","CALVIN, SIMPLER","Language, Vision","Open-X-Embodiment, Something-Something","T5, VQ-VAE, ViT","Autoregressive, Continuous","ビデオから抽出されたLatent Motion Tokenを「モーション言語」として使用し、GPTベースの事前学習とco-fine-tuningを通じてデータ効率の良いロボット操作ポリシーを学習するエンドツーエンドのフレームワークを提案。
SIMPLER、CALVIN、実世界のFANUCロボットにおいて優れた性能と汎化能力を実証。"
"End-to-End, Prediction",TraceVLA,TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies,ICLR,"Microsoft, University of Maryland",https://arxiv.org/abs/2412.10345,https://tracevla.github.io/,Manipulation,Tabletop,"Simulation, WidowX",Supervised,SIMPLER,"Language, Vision","BridgeV2, Open-X-Embodiment, RT-1","OpenVLA, Phi3-Vision","Autoregressive, Discrete Token",過去の動作軌跡を画像上に重ねるVisual Trace Prompting手法により、VLAモデルの空間・時間認識を強化し、操作タスクの汎化性能を向上させました。TraceVLAは、シミュレーターと実機ロボットの両方で、既存モデル（OpenVLA）に比べて優れた性能を実証しています。
"Device, End-to-End",RUMs,Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments,,"Meta, New York University",https://arxiv.org/abs/2409.05865,https://robotutilitymodels.com/,Manipulation,"Indoor, Kitchen, Tabletop","Hello Stretch, xArm",Supervised,,"Depth, Language, Proprioception, Vision",,HRP,"Autoregressive, Discrete Token",多様な環境で収集したデモデータを用いて多模態ポリシーを学習し、mLLMベースの失敗検出と再試行機能を備えたゼロショット汎用ポリシーRUMsを提案。Stick-v2ツールでデータ収集を効率化し、Hello Robot Stretchなどへの成功した展開を実現。
Affordance,RoboPoint,RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics,CoRL,"Allen Institute for AI, NVIDIA, University of Washington",https://arxiv.org/abs/2406.10721,https://robo-point.github.io/,"Manipulation, Navigation","Indoor, Tabletop","Franka Emika Panda, Simulation, YouBot",Supervised,"GQA, RoboRefIt, VQA-v2, Where2Place","Depth, Language, Vision","LLaVA Instruction, LVIS, RoboRefIt, WHERE2PLACE","CLIP, Vicuna","Autoregressive, Discrete Token",言語ベースの空間指示を用いて画像内の操作ポイントを予測する**point-based VLM (RoboPoint)**を提案し、自動合成データ生成と指示チューニングにより、実際の操作、ナビゲーション、ARにおいて高い精度を実現する。
"Audio, Hierarchical, Special",Yell At Your Robot,Yell At Your Robot: Improving On-the-Fly from Language Corrections,RSS,"Stanford University, UC Berkeley",https://arxiv.org/abs/2403.12910,https://yay-robot.github.io/,"Bi-manual, Manipulation",Tabletop,ALOHA,Supervised,,"Audio, Language, Proprioception, Vision",,"CLIP, DistilBERT, Whisper",Continuous,ユーザーの声のフィードバックを活用し、高水準の言語ポリシーをリアルタイムで修正し継続的に改善する階層的ロボット制御フレームワーク（YAY Robot）を提案する。ALOHA二腕ロボットを用いた実際の操作実験において、言語ベースの修正のみにより20％以上の性能向上を達成した。
Special,BYOVLA,Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust,,Princeton University,https://arxiv.org/abs/2410.01971,https://aasherh.github.io/byovla/,Manipulation,"Kitchen, Tabletop",WidowX,,,"Language, Proprioception, Vision",BridgeV2,"GPT, GroundingDINO, Octo, OpenVLA, SAM2",,BYOVLAは、視覚-言語-行動モデルの視覚的感度を低減するため、実行時にタスクと無関係な領域を特定し、感度の高い部分のみを最小限に修正する軽量な手法です。モデル自体を変更することなく、ディストラクターに対して頑強なポリシーの実行が可能です。
End-to-End,Actra,Actra: Optimized Transformer Architecture for Vision-Language-Action Models in Robot Learning,,China,https://arxiv.org/abs/2408.01147,,Manipulation,Tabletop,Simulation,"Self-Supervised, Supervised","FrankaKitchen, Maniskill2, Push-T","Language, Vision",,T5,Parallel Decoding,Actraは、セグメント単位のTrajectory AttentionとAction Queryを活用し、従来の因果関係に基づくVLAの限界を克服した、モダリティ間の整合性を強化したVLA専用のトランスフォーマーです。これにより、多様なロボット操作環境において最先端の性能を実現します。
"End-to-End, Prediction",GR-1,Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation,,ByteDance Research,https://arxiv.org/abs/2312.13139,https://gr1-manipulation.github.io/,Manipulation,Tabletop,"Kinova Gen2, Simulation","Self-Supervised, Supervised",CALVIN,"Language, Proprioception, Vision",Ego4D,CLIP,"Autoregressive, Continuous",Video Generation Modelとしてpre-trainingされたモデルをfine-tuningしたらVLAの性能が良くなる
"End-to-End, Tactile",TLA,TLA: Tactile-Language-Action Model for Contact-Rich Manipulation,,China,https://arxiv.org/abs/2503.08548,https://sites.google.com/view/tactile-language-action/,"Manipulation, Peg-in-Hole","Assembly, Tabletop","Simulation, UR5","LoRA, Supervised",,"Language, Tactile",TLA Dataset,Qwen2-VL,"Autoregressive, Discrete Token","Qwen2-7BのViTにTactileを入れ整数値の言語tokenを出力, 小数点に直してactionに. LoRA"
End-to-End,ECoT,Robotic Control via Embodied Chain-of-Thought Reasoning,CoRL,"Stanford University, UC Berkeley",https://arxiv.org/abs/2407.08693,https://embodied-cot.github.io/,Manipulation,Tabletop,WidowX,Supervised,,"Language, Proprioception, Vision",BridgeV2,"DINOv2, Gemini, GroundingDINO, LLaMA2, OpenVLA, SAM, SigLIP","Autoregressive, Discrete Token","OpenVLAをCoTデータでfinetuning. 画像と言語から直接actionではなく, scene descriptionやobject detectionの結果などもその前にちまちま入れて学習しCoTを実現"
"Application-Oriented, Hierarchical, RL",NaVILA,NaVILA: Legged Robot Vision-Language-Action Model for Navigation,RSS,"NVIDIA, UC San Diego",https://arxiv.org/abs/2412.04453,https://navila-bot.github.io/,Navigation,"Indoor, Outdoor","Booster T1, Unitree Go2, Unitree H1","RL, Supervised","R2R-CE, RxR-CE, ScanQA, VLN-CE","Language, PointCloud, Proprioception, Vision","R2R-CE, RxR-CE, ScanQA, Youtube",VILA,Continuous,"自然言語で中間レベルの行動（例：「右に30度回転、1m前進」）を生成するVLAと、これを実際の関節制御に変換するRLロコモーションポリシーを組み合わせた2段階のVLNフレームワークを提案する。
この構造により、従来の手法と比較して、実際の環境およびシミュレーション環境の両方で成功率、汎用性、頑健性を大幅に向上させた。"
"Application-Oriented, End-to-End",RoboNurse-VLA,RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model,,"IIT, The University of Hong Kong",https://arxiv.org/abs/2409.19590,https://robonurse-vla.github.io/,"Human-Cooperation, Manipulation","Surgical, Tabletop",UR5,"LoRA, Supervised",,"Audio, Language, Proprioception, Vision",,"ASR, LLaMA2, OpenVLA, SAM2","Autoregressive, Discrete Token","SAM2ベースのビジョンモジュールとLlama2言語モデルを統合したVLAモデルを用いて、音声コマンドに従って複雑な手術ツールを検出・把持・伝達するロボット看護師システムを開発し、既存のSOTAモデルと比較してすべての実験で圧倒的な成功率を達成しました。
主な手法：YOLOv8+SAM2による視覚分割、Llama2のファインチューニング（LORA）による言語-行動予測、行動のディスクリタライズとトークナイジング、実際のUR5ロボットを用いた実験。"
"Application-Oriented, Hierarchical, Planning, Special",Mobility VLA,Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs,,Google DeepMind,https://arxiv.org/abs/2407.07775,,Navigation,Indoor,Google EDR,,,"Language, Vision",,Gemini,Optimization,"Mobility VLAは、デモツアー動画とマルチモーダル（テキスト+画像）の指示を入力として、Long-context VLM（Gemini 1.5 Pro）とオフライントポロジカルグラフ（COLMAPベース）を組み合わせたハイブリッドポリシーで目標位置を推論し、実際の環境内で堅牢なナビゲーションを実行します。
この構造は、従来のVLM単独方式よりも複雑な指示や状況依存のマルチモーダル命令において優れた成功率を示し、環境に応じてスマートフォン動画のみがあれば簡単に適用可能です。"
"Application-Oriented, End-to-End",Uni-NaVid,Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks,RSS,Peking University,https://arxiv.org/abs/2412.06224,https://pku-epic.github.io/Uni-NaVid/,Navigation,Indoor,"Simulation, Unitree Go2",Supervised,"HM3D, HM3D-OVON, MP3D-EQA, OpenEQA, R2R-CE, RxR-CE, VLN-CE","Language, Vision","HM3D, HM3D-OVON, MP3D-EQA, R2R-CE, RxR-CE, ScanQA, VLN-CE","EVA-CLIP, Vicuna","Autoregressive, Discrete Token","多様なロボットナビゲーションタスク（ビジョン-言語、オブジェクト探索、質問応答、人間追跡）を統合し、RGBビデオと言語コマンドのみでリアルタイム行動をエンドツーエンドで出力するモデルと、効率的なトークンマージ技術および360万サンプルの大規模データによりSOTA性能を達成しました。
（手法：オンライントークンマージによる時空間情報圧縮、マルチタスク大規模データ共同トレーニング、現実世界でのゼロショット適用可能）
."
"Application-Oriented, Dataset, End-to-End",CoVLA,CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving,WACV,"Keio, NII, The University of Tokyo, Turing, University of Tsukuba",https://arxiv.org/abs/2408.10845,https://turingmotors.github.io/covla-ad/,"Autonomous Driving, Navigation","Autonomous Driving, Outdoor",Vehicle,Supervised,,"Language, Proprioception, Vision",CoVLA Dataset,"CLIP, LLaMA2","Autoregressive, Continuous","実際の道路走行動画を基に、自動生成されたフレームごとの言語キャプションと未来の軌跡を含む大規模なVLAデータセットCoVLAと、これを利用したエンドツーエンドの自律走行VLAモデルCoVLA-Agentを提案した。
自動化されたセンサー融合、ルール+VLMベースのキャプション生成などによりデータ収集/ラベリングを拡張し、一貫した言語-行動予測性能を実験で検証した。"
"Application-Oriented, End-to-End",OpenDriveVLA,OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model,,TUM,https://arxiv.org/abs/2503.23463,https://drivevla.github.io/,"Autonomous Driving, Navigation","Autonomous Driving, Outdoor",Vehicle,Supervised,,"Depth, Language, PointCloud, Proprioception, Vision",nuScenes,Qwen2.5-VL,"Autoregressive, Discrete Token","OpenDriveVLAは、2D/3D環境認識とドライバーのコマンドを階層的ビジョン-言語アラインメントおよびエージェント-環境-車両相互作用モジュールを通じて、LLMベースでエンドツーエンドの走行軌道を直接生成するモデルであり、nuScenesデータセットでSOTAを達成しました。
（主要な手法：2D/3Dトークンの言語埋め込みアラインメント、オートリグレッション相互作用、運転指示のチューニング、共同エンドツーエンド軌道計画）
."
"Application-Oriented, End-to-End",ORION,ORION: A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation,,China,https://arxiv.org/abs/2503.19755,https://xiaomi-mlab.github.io/Orion/,"Autonomous Driving, Navigation","Autonomous Driving, Outdoor",Simulation,"LoRA, Supervised",Bench2Drive,"Language, Vision",Bench2Drive,"EVA-02, Vicuna",Conditional VAE,ORIONはQT-Formerで長期的な文脈を要約し、LLMで運転状況を推論し、VAEベースの生成型プランナーで推論空間と行動空間を一致させて、エンドツーエンドで数値的な走行軌跡を生成します。Bench2Drive実験において、既存のSOTAと比較してDS +14.28%、SR +19.61%の大幅な性能向上を実現しました。
"Application-Oriented, Hierarchical, Planning",UAV-VLA,UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation,,Russia,https://arxiv.org/abs/2501.05014,,Navigation,Outdoor,UAV,,,"Language, Vision",,"GPT, Molmo",Primitive,"衛星画像と自然言語コマンドを入力すると、ビジョン・ランゲージ・モデルとGPTを組み合わせることで、ドローンの飛行経路および行動を自動的に生成するUAV-VLAシステムを提案する。
このシステムは、人間よりも6.5倍速く、22%長い経路でミッションを完了し、衛星ベースの言語指示によるUAV経路生成のベンチマークと手法（GPT+VLMの組み合わせ、ゼロショット）も併せて提示する。
。"
"Application-Oriented, End-to-End",CombatVLA,CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games,ICCV,China,https://arxiv.org/abs/2503.09527,https://combatvla.github.io/,Game,Game,Simulation,Supervised,,"Language, Vision",,Qwen2.5-VL,Primitive,"CombatVLAは、人間プレイヤーのデータに基づくAoTシーケンスをプログレッシブ学習で学習し、3D戦闘ゲームにおいてリアルタイムの高速戦闘行動予測が可能な3B VLAモデルを提案します。
トランクトークンに基づく推論の短縮、アクションアラインメント/コントラスト損失などにより、戦術的推論と実行効率を同時に実現しています。"
"Application-Oriented, End-to-End",JARVIS-VLA,JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse,,Peking University,https://arxiv.org/abs/2503.16365,https://craftjarvis.github.io/JarvisVLA/,Game,Game,Simulation,"Self-Supervised, Supervised",,"Language, Vision",,"LLaVA-Next, Qwen2-VL","Autoregressive, Discrete Token","ビジョン-言語モデルに動作学習を実施する前に、ビジョン-言語のポストトレーニング（post-training）を適用し、環境理解と空間的整列能力を強化し、その後、動作模倣学習を通じてマインクラフト内の1,000を超えるタスクでSOTA性能を達成したVLAモデルを提案する。
主要な手法：ビジョン-言語ベースの自己教師付き後処理（ActVLP）とトラジェクトリー模倣学習の組み合わせ。"
"Application-Oriented, Hierarchical",Shake-VLA,Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing,,Russia,https://arxiv.org/abs/2501.06919,,"Bi-manual, Manipulation",Tabletop,UR3,,,"Audio, Language, Proprioception, Tactile, Vision",,"GPT, Whisper",Primitive,"Shake-VLAは、ビジョン・言語・アクション統合AIとRAG、異常検出、力センサー計測を組み合わせ、両手ロボットによる自動カクテル製造を実現したシステムです。

YOLOv8、Whisper-1、FAISS、GPT-4oなど最新のAI技術をモジュール式に連携させ、実環境でも100%の成功率でレシピの実行と材料の代替処理を処理します。"
"Hierarchical, Planning",DexGraspVLA,DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping,,"HKUST, Peking University",https://arxiv.org/abs/2502.20900,https://dexgraspvla.github.io/,"Dexterous Hand, Grasp, Manipulation",Tabletop,"PsiBot G0-R, Realman",Supervised,,"Language, Proprioception, Vision",,"DINOv2, Qwen2.5-VL, SAM",Diffusion,"DexGraspVLAは、ビジョン-言語基礎モデルと拡散ベースのポリシーを組み合わせることで、多様な環境/言語/オブジェクトにおいて汎化可能なデクスターラスなグリッピングを実現した階層的VLAフレームワークです。

言語コマンドを高度なVLMがタスクに分解・指示し、低レベルの拡散ポリシーがドメイン不変表現に基づいてアクションを実行することで、ゼロショット状況でも90%以上の成功率を示しました
."
"End-to-End, Humanoid, RL",Humanoid-VLA,Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration,,"China, Zhejiang University",https://arxiv.org/abs/2502.14795,,"Manipulation, Navigation, Whole-Body Control",Indoor,Unitree G1,"RL, Self-Supervised, Supervised","HumanML3D, Text-to-Motion","Language, Proprioception, Vision","AMASS, Human3.6M, HumanML3D, Motion-X",LLaMA3,"Autoregressive, Discrete Token","Humanoid-VLAは、言語と動作の事前アラインメント、1人称視覚統合、自己監督型データ拡張を組み合わせることで、汎用的な自律型ヒューマノイド制御を実現した最初のVLAフレームワークです。
大規模な非注釈付きビデオデータ拡張、クロスアテンションベースの視覚・言語・行動統合、全身強化学習制御ポリシーなどが主要な手法です。"
End-to-End,QUAR-VLA,QUAR-VLA: Vision-Language-Action Model for Quadruped Robots,ECCV,"China, Zhejiang University",https://arxiv.org/abs/2312.14457,https://sites.google.com/view/quar-vla/quar-vla-eccv24,"Manipulation, Navigation, Whole-Body Control",Indoor,"Simulation, WR-2",Supervised,,"Language, PointCloud, Vision",,Fuyu,"Autoregressive, Discrete Token","視覚情報と自然言語コマンドを同時に受け入れて高次元アクションを生成する4足ロボット用のVision-Language-Action（QUAR-VLA）フレームワーク、大規模マルチタスクデータセット（QUARD）、トランスフォーマーベースのポリシーモデル（QUART）を提案。
多様なタスクと未学習のコマンド/物体に対する強い汎化性能とsim2real性能を示し、シミュレーションと実世界のデータ統合によるco-training、事前学習済みマルチモーダルLLMに基づくアクショントークン化が核心的な手法である。"
"Application-Oriented, End-to-End",RT-Sketch,RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches,CoRL,"Google DeepMind, Stanford University",https://arxiv.org/abs/2403.02709,https://rt-sketch.github.io/,Manipulation,Tabletop,Google EDR,Supervised,,Vision,RT-1,RT-1,"Discrete Token, Parallel Decoding",手描きスケッチを目標として入力を受け、行動を予測するRT-Sketchモデルを提案。画像からスケッチへの変換ネットワークと大規模なデモデータと組み合わせ、エンドツーエンドの模倣学習で学習させる。スケッチベースのポリシーは、言語/画像ベースのポリシーよりも曖昧さや障害物に対してより頑強であり、多様なスケッチスタイルに対しても堅牢である。
"3D, End-to-End",StructDiffusion,StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects,RSS,"Georgia Tech, MIT, Meta, NVIDIA",https://arxiv.org/abs/2211.04604,https://structdiffusion.github.io/,Manipulation,Tabletop,"Kinova jaco, Simulation",Supervised,,"Language, PointCloud",,PointCloudTransformer,Diffusion,"StructDiffusionは、言語コマンドと未知の物体のポイントクラウドのみを使用して、拡散モデルと判別器を組み合わせることで物理的に有効な3D構造を生成する方法を提案し、既存手法に比べて16%以上の成功率向上を実現しています。
手法：オブジェクト中心のマルチモーダルトランスフォーマー + 拡散ベースの目標サンプリング + 衝突判別器により現実性と一貫性を確保。"
"End-to-End, RL",MoRE,MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models,,"HKUST, Zhejiang University",https://arxiv.org/abs/2503.08007,,"Navigation, Whole-Body Control",Indoor,"Simulation, Unitree Go2","LoRA, RL",QUARD,"Language, Proprioception, Vision",QUARD,Fuyu,"Autoregressive, Discrete Token","複数のLoRAモジュールをexpertとして使用したsparse MoE構造とRL objectiveを組み合わせることで、多様な品質のデータ効率的に活用し、多機能4足VLAロボットの汎用性と汎化性能を大幅に向上させる。
Fuyu 8Bバックボーン+MixLoRAベースのsparse Mixture-of-Experts構造、オフラインRL（Q-ラーニング）objective、サブオプティマルデータ統合手法が核心。"
"3D, Dataset, Evaluation",VLA-3D,VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation,RSS,Carnegie Mellon University,https://arxiv.org/abs/2411.03540,https://github.com/HaochenZ11/VLA-3D,Navigation,Indoor,,,,"Language, PointCloud, Vision",,,,"実際の3Dスキャンに基づく大規模な室内シーンデータセット（VLA-3D）を構築し、ポイントクラウド/シーングラフ/空間関係/自然言語参照文を自動生成することで、3D意味理解および言語ベースのロボットナビゲーション研究のためのベンチマークを提案する。
主要な手法：複数の室内3Dデータセットのポイントクラウドとオブジェクト/空間情報を整合・クラスタリング・グラフ化し、ヒューリスティックベースのテンプレートを用いて空間関係に応じた参照文を自動生成する。"
"3D, Application-Oriented, End-to-End, Prediction",OccLLaMA,OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving,,Tsinghua University,https://arxiv.org/abs/2409.03272,https://vilonge.github.io/OccLLaMA_Page/,"Autonomous Driving, Navigation",Autonomous Driving,Simulation,"LoRA, Self-Supervised",,Language,nuScenes,"LLaMA2, LLaMA3","Autoregressive, Discrete Token","セマンティック・オキュパンスを視覚トークンとして、言語・行動トークンと統合し、LLaMAベースのオートレグレッシブモデルが予測・計画・質問応答などの多様な自律走行タスクを同時に実行するワールドモデルを提案。
VQVAEベースのシーン・トークナイザー、統合トークン辞書、空間的アテンションなどにより、3D視覚・言語・行動マルチモーダル学習および予測性能を向上させる。"
"Application-Oriented, End-to-End",RaceVLA,RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour,,Russia,https://arxiv.org/abs/2503.02572,https://racevla.github.io/,Navigation,Outdoor,UAV,"LoRA, Supervised",,"Language, Proprioception, Vision",,OpenVLA,"Autoregressive, Continuous","RaceVLAは、FPV映像と自然言語コマンドを入力してリアルタイムでドローンを制御するVLAベースの自律レーシングシステムであり、OpenVLA・RT-2よりも高い汎化性能と高速・安定した走行を示しました。
手法：OpenVLA-7bにLoRAでレーシングドローンデータセットをファインチューニングし、4D速度制御信号を生成。ROS・実機ドローンにリアルタイムで適用。"
"Failure-Recovery, Hierarchical, Prediction",FOREWARN,From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment,,"Carnegie Mellon University, UC Berkeley",https://arxiv.org/abs/2502.01828,https://yilin-wu98.github.io/forewarn/,Manipulation,Tabletop,Franka Emika Panda,"LoRA, Self-Supervised, Supervised",,"Language, Proprioception, Vision",,"Dreamer, LLaMA3",Diffusion,"ロボット政策の候補行動シーケンスを潜在世界モデル（Latent World Model）で予測し、VLMを潜在空間に微調整（Fine-tune）させて行動結果を自然言語で評価・選択するVLM-in-the-loopポリシーステアリングフレームワーク（FOREWARN）を提案する。
これにより、多様なタスクとユーザー要件に合わせてリアルタイムでロボットの行動を安全かつ柔軟にステアリングすることが可能となる。"
"Application-Oriented, End-to-End, Hierarchical",CognitiveDrone,CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs,,Russia,https://arxiv.org/abs/2503.01378,https://cognitivedrone.github.io/,Navigation,Outdoor,Simulation,"LoRA, Supervised",CognitiveDroneBench,"Language, Vision",,"OpenVLA, Qwen2.5-VL","Autoregressive, Discrete Token","シミュレーションに基づく8,000件以上の経路データを活用し、OpenVLAをLoRAで学習させ、推論モジュール（Qwen2.5-VL）を追加したCognitiveDrone-R1は、認知タスクにおいて従来のRaceVLA比で最大45.6%の性能向上を実現しました。
VLAモデルに基づくUAV認知制御およびこれを定量評価可能なベンチマーク（CognitiveDroneBench）を初めて提案しました。"
"Hierarchical, RL",DexTOG,DexTOG: Learning Task-Oriented Dexterous Grasp with Language,RAL,Shanghai Jiao Tong University,https://arxiv.org/abs/2504.04573,,"Dexterous Hand, Grasp",Tabletop,Simulation,RL,,"Language, PointCloud, Proprioception, Vision",,"GPT, PointNet",Diffusion,言語ベースの条件下で多関節手のタスク指向型把持を生成する拡散ベースのモデルDiffuTOGと、強化学習ベースの自動データ生成器DexTOGを提案する。規則フィルタリング-拡散モデル-強化学習を通じたデータセットの生成およびRLで検証されたTOGの性能を達成した。
"End-to-End, Evaluation, Hierarchical, Humanoid",LeVERB,LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction,,"Carnegie Mellon University, Simon Fraser University, UC Berkeley",https://arxiv.org/abs/2506.13751,https://ember-lab-berkeley.github.io/LeVERB-Website/,"Navigation, Whole-Body Control",Indoor,"Simulation, Unitree G1","RL, Supervised",LeVERB-Bench,"Language, Proprioception, Vision",,SigLIP,"Autoregressive, Continuous","LeVERBは、CVAEを基盤とした階層的潜在ビジョン・言語・アクションフレームワークであり、高次元なビジョン・言語入力を潜在行動辞書に変換し、低次元なWBCポリシーに伝達することで、合成データのみを用いてヒューマノイドのゼロショット全身制御を実現しました。
フォトリアリズム、シーン・コマンドのランダム化、シミュレーションから現実への移行のための新しいベンチマークと大規模データ生成パイプラインも同時に提案しています。"
Augmentation,DIAL,Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models,RSS,Robotics at Google,https://arxiv.org/abs/2211.11736,https://instructionaugmentation.github.io/,Manipulation,"Kitchen, Tabletop",Google EDR,Supervised,,"Language, Proprioception, Vision",,"CLIP, RT-1","Autoregressive, Discrete Token",DIALは、少量の人間の注釈でCLIPを微調整し、大規模なロボットデモを自動言語再表記し、この拡張データで学習したポリシーが60の未指示課題で40%以上のパフォーマンスを向上させた。 つまり、高価な言語ラベルがなくても、インターネットサイズのVLM知識でロボットの言語理解・一般化を大幅に拡張できることを示している。
Augmentation,CACTI,CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning,CoRL,"Carnegie Mellon University, Columbia University, Meta",https://arxiv.org/abs/2212.05711,https://cacti-framework.github.io/,Manipulation,"Kitchen, Tabletop",Franka Emika Panda,Supervised,,"Language, Proprioception, Vision",,R3M,"Autoregressive, Continuous",CACTIは、「Collect→Augment→Compress→TraIn」の4つのステップで小規模なデモを生成モデル拡張・事前学習エンベデッドと組み合わせ、1つのビジョンポリシーで10（実）〜18（シミュレート）個のキッチン操作を行うように拡張しました。Stable Diffusionベースの拡張とR3M/MoCoエンベデッドにより、データ効率と未見シーンの一般化が大幅に向上します。
End-to-End,RT-1,RT-1: Robotics Transformer for Real-World Control at Scale,RSS,Google Research,https://arxiv.org/abs/2212.06817,https://robotics-transformer1.github.io/,"Manipulation, Move Base","Kitchen, Tabletop",Google EDR,Supervised,,"Language, Proprioception, Vision",RT-1,"RT-1, Universal Sentence Encoder","Discrete Token, Parallel Decoding",EfficientNet + FiLM conditioning + TokenLearner + Transformer
Prediction,UniPi,Learning Universal Policies via Text-Guided Video Generation,NeurIPS,"Google Brain, MIT, UC Berkeley",https://arxiv.org/abs/2302.00111,https://universal-policy.github.io/unipi/,Manipulation,Tabletop,Simulation,"Self-Supervised, Supervised",,"Language, Proprioception, Vision","BridgeV1, LAION",T5,"Continuous, Image Generation",動画生成モデルから逆運動学モデルを学習して動作実行
Prediction,GEVRM,GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation,ICLR,Zhejiang University,https://arxiv.org/abs/2502.09268,,Manipulation,Tabletop,"Simulation, UR5","Self-Supervised, Supervised",CALVIN,"Language, Vision","BridgeV2, CALVIN",T5,Diffusion,GEVRMはIMC原理を基盤に、テキスト条件付きビデオ生成と状態整列を組み合わせることで、外部干渉にも耐えられるロボットポリシーを学習します。視覚的に豊かな目標生成と対比学習に基づくポリシーにより、CALVINおよび現実世界のロボット操作において最先端（SOTA）の性能を達成しました。
Prediction,Dreamitate,Dreamitate: Real-World Visuomotor Policy Learning via Video Generation,CoRL,"Columbia University, Stanford University, Toyota Research Institute",https://arxiv.org/abs/2406.16862,https://dreamitate.cs.columbia.edu/,"Bi-manual, Manipulation",Tabletop,"UR5, xArm",Supervised,,Vision,,"MegaPose, Stable Video Diffusion",Image Generation,Stable VIdeo DiffusionをFinetuningしてロボットの動画を作り、動画からロボットの姿勢(ツールの姿勢)を推定してactionにする
Prediction,Track2Act,Track2Act: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation,ECCV,"Carnegie Mellon University, Meta, University of Washington",https://arxiv.org/abs/2405.01527,https://homangab.github.io/track2act/,Manipulation,"Indoor, Tabletop",Spot,"Self-Supervised, Supervised",,Vision,"BridgeV2, EPIC-Kitchens, RT-1, Something-Something",,Diffusion,"ウェブから収集した人間/ロボットの動画から2Dポイントトラックを予測し、これを3D剛体変換に変換してロボット操作計画を生成した後、少量のロボットデータで残差ポリシーを学習し、一般化された操作を可能にするフレームワーク。
手法：DiTベースのポイントトラック拡散モデル + オープンループ剛体変換 + 限定データを用いた残差補正。"
Prediction,Gen2Act,Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation,,"Carnegie Mellon University, Google DeepMind, Stanford University",https://arxiv.org/abs/2409.16283,https://homangab.github.io/gen2act/,Manipulation,"Indoor, Kitchen, Tabletop",Google EDR,Supervised,,"Language, Vision",RT-1,"Gemini, TAP-VID, VideoPoet",Discrete Token,"手法：人間動作ビデオ生成（言語条件）＋人間からロボットへの翻訳ポリシー（ポイントトラック予測補助）。
2行要約：
ウェブデータに基づく人間動作ビデオ生成モデルを活用し、言語目標から人間ビデオを生成し、これを入力として受け取るロボットポリシーを学習することで、新しいタスクや未知の物体/動作に対しても汎用化されたロボット操作を実現する。
手法：生成ビデオ条件ポリシー＋ポイントトラック予測補助損失。"
Augmentation,GenAug,GenAug: Retargeting behaviors to unseen situations via Generative Augmentation,RSS,"Meta, University of Washington",https://arxiv.org/abs/2302.06671,https://genaug.github.io/,Manipulation,Tabletop,xArm,Supervised,RAVENS,"Language, Vision",,CLIPort,"Heatmap, Primitive",ウェブスケール生成モデルを用いて、実際のデモ数枚をセマンティックに100倍以上増強し、単純なピックアンドプレース操作でも新しい環境・オブジェクトへの一般化性能を最大40%p向上させました。鍵となるのは、行動不変を維持したまま、オブジェクト・背景・障害物を多様化するdepth-guidedテキスト・画像生成である。
Augmentation,ROSIE,Scaling Robot Learning with Semantically Imagined Experience,RSS,"Google Research, Robotics at Google",https://arxiv.org/abs/2302.11550,https://diffusion-rosie.github.io/,Manipulation,"Indoor, Kitchen, Tabletop",Google EDR,Supervised,,"Language, Proprioception, Vision",RT-1,"Imagen, OwL-ViT, RT-1, T5","Discrete Token, Parallel Decoding",テキスト・ガイド拡散・インペインティングにより、既存のロボットデモを意味論的に変形・増幅し、データ収集なしに新しいタスク・環境に一般化可能なポリシーと成功検出器を学習した。ROSIEはnovel taskの成功率・堅牢性を大幅に向上させるが、物理的なモーションの多様性及び連続性の補強は今後の課題である。
"Augmentation, End-to-End",MOO,Open-World Object Manipulation using Pre-trained Vision-Language Models,CoRL,Google DeepMind,https://arxiv.org/abs/2303.00905,https://robot-moo.github.io/,Manipulation,"Indoor, Kitchen, Tabletop",Google EDR,Supervised,,"Language, Proprioception, Vision",RT-1,"OwL-ViT, RT-1","Discrete Token, Parallel Decoding","事前学習VLMとRT-1ベースのポリシーを組み合わせたMOOは、テンプレート指示だけで未知のオブジェクトを操作する実ロボットゼロショット性能を実証しました。シンプルなピクセルマスクを活用することでパイプラインの複雑さを低減しながら、1,472回の実験で従来手法と比較して50%以上の成功率を達成します。"
"Augmentation, Training",ReMix,Re-Mix: Optimizing Data Mixtures for Large Scale Imitation Learning,CoRL,"Stanford University, UC Berkeley",https://arxiv.org/abs/2408.14037,https://github.com/jhejna/remix,Manipulation,Tabletop,"Franka Emika Panda, WidowX",Supervised,,Vision,"BridgeV2, Open-X-Embodiment",,Diffusion,Re-Mixは、DRO(Distributionally Robust Optimization)にアクション正規化・離散化・早期終了を組み合わせ、ロボットデータ領域の重みを自動学習し、RT-X基準で均等比38％・エキスパート比32％の性能を向上させる。25%のみのデータでも同じ性能を維持し、学習コストも削減する。
"Augmentation, RL",Video2Policy,Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos,,"Shanghai Artificial Intelligence Laboratory, Tsinghua University, UC Berkeley, UC San Diego",https://arxiv.org/abs/2502.09886,https://yewr.github.io/video2policy/,Manipulation,Tabletop,"Franka Emika Panda, Simulation",RL,,"Depth, Language, PointCloud, Proprioception, Vision",Something-Something,"FoundationPose, GPT, GroundingDINO, InstantMesh, SAM2, UniDepth",Continuous,インターネットRGB映像を自動的にシミュレーションタスクに変換し、VLM + RL反復でポリシーを学習して汎用操作ポリシーを構築・実世界転移まで成功。視覚・補正反映モジュールの品質が性能向上の鍵となる。
Dataset,DROID,DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset,RSS,"Carnegie Mellon University, Columbia University, Google DeepMind, KAIST, Princeton University, Stanford University, The University of Edinburgh, Toyota Research Institute, UC Berkeley, UC Davis, UC San Diego, University of Pennsylvania, University of Texas at Austin, University of Washington",https://arxiv.org/abs/2403.12945,https://droid-dataset.github.io/,Manipulation,"Indoor, Kitchen, Tabletop",Franka Emika Panda,Supervised,,"Depth, Language, Proprioception, Vision",DROID,DistilBERT,Diffusion,Franka Emika Pandaロボットを用いたDROID Robot Platformによるin-the-wildの大規模ロボット操作データセット
Dataset,Robocasa,RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots,RSS,"NVIDIA, University of Texas at Austin",https://arxiv.org/abs/2406.02523,https://robocasa.ai/,Manipulation,Kitchen,Simulation,,RoboCasa,,RoboCasa,,,RoboCasaは生成型AIを活用し、120のキッチンと10万以上のシミュレーション軌跡を提供し、ロボット一般化のための最大規模のホームシミュレータです。機械が生成したデータで人間によるデモと比較して1.7倍の性能を達成し、実際のキッチンにも移行可能であることを実証しました。
Dataset,ManiSkill3,ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI,RSS,"Carnegie Mellon University, Tsinghua University, UC San Diego",https://arxiv.org/abs/2410.00425,https://www.maniskill.ai/,"Manipulation, Navigation, Whole-Body Control","Indoor, Kitchen, Tabletop",Simulation,,Maniskill3,,Maniskill3,,,ManiSkill 3は、GPU並列物理・レンダリング・ヘテロジニアスシミュレーションを統合し、操作研究を数万FPSで加速し、12種類の課題・大規模デモ・VRテレオプを含む総合的なエコシステムを提供します。これにより、シミュレーションベースの学習が「分単位」で現実に転移する実用的なロボット操作研究が可能になりました。
Dataset,Ego-Exo 4D,Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives,CVPR,"Carnegie Mellon University, Georgia Tech, Meta, National University of Singapore, Simon Fraser University, The University of Tokyo, UC Berkeley, University of Bristol, University of Illinois Urbana-Champaign, University of Minnesota, University of Pennsylvania, University of Texas at Austin",https://arxiv.org/abs/2311.18259,https://ego-exo4d-data.org/,,"Assembly, Indoor, Industrial, Kitchen, Outdoor, Tabletop",Human,,,"Audio, Depth, Language, PointCloud, Proprioception, Vision",Ego-Exo4D,,,"Ego-Exo4Dは、740人の実際の熟練活動を同期化された1-3人称マルチセンサー映像（1,286時間）で記録し、関係・認識・熟練度・熟練度・ポーズの4つのベンチマークを提供する世界最大規模のEgo-Exoデータセットです。これは、AR/ロボットのための視点変換学習と自動コーチング研究の基礎を築きます。"
"Device, Open-Source",Mobile ALOHA,Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation,CoRL,Stanford University,https://arxiv.org/abs/2401.02117,https://mobile-aloha.github.io/,"Bi-manual, Manipulation, Move Base","Indoor, Kitchen, Tabletop",ALOHA,Supervised,,"Proprioception, Vision",,,Continuous,低コストのモバイル両腕ロボットMobile ALOHAと静的データコトレーニングレシピを提案し、20-50回のデモだけで7種の家庭・環境課題を80%以上成功させた。これは、移動・両腕・全身の協調を要求する実作業においても、単純なBC系学習が効果的であることを示している。
"Device, Open-Source",Dobb-E,On Bringing Robots Home,,"Meta, New York University",https://arxiv.org/abs/2311.16098,https://dobb-e.com/,Manipulation,Indoor,Hello Stretch,"Self-Supervised, Supervised",,"Depth, Vision",Homes of New York,,Continuous,Stick + iPhoneで低コストのデモを集め、HPR (Home Pretrained Representations)を学習し、30分以内にファインチューニングして、実際の家庭用ロボット学習フレームワークDobb-Eを提示し、109個の課題を81%成功させた。実験とコード・データ・ハードウェアをすべて公開し、家庭用ロボット研究の再現性と拡張性を大幅に向上させた。
"Device, Open-Source",UMI,Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots,RSS,"Columbia University, Stanford University, Toyota Research Institute",https://arxiv.org/abs/2402.10329,https://umi-gripper.github.io/,"Bi-manual, Manipulation","Kitchen, Tabletop","Franka Emika Panda, UMI, UR5",Supervised,,"Proprioception, Vision",,CLIP,Diffusion,データ収集のためのGripperデバイスを開発し、人が簡単にロボット操作学習のためのデータ収集ができるようなフレームワークを提案
Device,DexUMI,DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation,RSS,"Carnegie Mellon University, Columbia University, NVIDIA, Stanford University",https://arxiv.org/abs/2505.21864,https://dex-umi.github.io/,"Bi-manual, Dexterous Hand, Manipulation","Kitchen, Tabletop","DexUMI, Inspire Hand, UR5, XHand",Supervised,,"Proprioception, Tactile, Vision",,"DINOv2, ProPainter, SAM2",Continuous,ウェアラブル外骨格とインペインティングパイプラインにより、人間の手のデモンストレーションをロボット手に高効率・高精度に移行し、4つの実際の作業で平均86％の成功率を達成しました。DexUMIは、従来の遠隔操作に比べて3.2倍速いデータ収集と多種のロボット手の適用可能性を示している。
"Device, Humanoid, Open-Source",Open-TeleVision,Open-TeleVision: Teleoperation with Immersive Active Visual Feedback,CoRL,"MIT, UC San Diego",https://arxiv.org/abs/2407.01512,https://robot-tv.github.io/,"Bi-manual, Dexterous Hand, Manipulation",Tabletop,"GR-1, Unitree H1",Supervised,,"Proprioception, Vision",,DINOv2,"Continuous, Parallel Decoding",VRベースのOpen-TeleVisionは、ロボットの頭部・カメラを能動的に制御しながらステレオ視界をストリーミングし、マルチフィンガー両手ヒューマノイドを遠隔から高精度に操作し、これにより収集したデモでモジュール式ILポリシーまで学習する。既存システムと比較して「遠隔+深度+多地」の統合を初めて達成した。
Device,DexWild,DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies,RSS,Carnegie Mellon University,https://arxiv.org/abs/2505.07813,https://dexwild.github.io/,"Bi-manual, Dexterous Hand, Manipulation","Indoor, Tabletop","Franka Emika Panda, LEAP Hand, xArm",Supervised,,"Proprioception, Vision",,,Diffusion,携帯型DexWild-Systemで収集した大量の人間のデモンストレーションと少量のロボットのデモンストレーションを一緒に学習し、ロボットが全く新しい環境・タスク・ハードウェアでも高い成功率を達成します。コ・トレーニング戦略により、未知の環境での成功率4倍、クロスエンボディメント5.8倍の向上を実現しました。
Device,DexCap,DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation,RSS,Stanford University,https://arxiv.org/abs/2403.07788,https://dex-cap.github.io/,"Bi-manual, Dexterous Hand, Manipulation",Tabletop,"Franka Emika Panda, LEAP Hand",Supervised,,"PointCloud, Proprioception, Vision",,,Diffusion,DEXCAPはハンドヘルドSLAM-EMFベースのハンドモキャップで遮蔽のない高精度の3Dデータを収集し、DEXILはそれをポイントクラウド+ディフュージョンポリシーで即座にロボットの手に移植し、複雑な操作を学習します。携帯性・データ品質のおかげで、30分間のデモだけで複数の実環境タスクを高い成功率で実行し、少量の人間による補正でさらに向上します。
Device,ACE,ACE: A Cross-Platform Visual-Exoskeletons System for Low-Cost Dexterous Teleoperation,CoRL,UC San Diego,https://arxiv.org/abs/2408.11805,https://ace-teleop.github.io/,"Bi-manual, Dexterous Hand, Manipulation","Indoor, Tabletop","Ability Hand, Franka Emika Panda, GR-1, Inspire Hand, Unitree B1, Unitree H1, Unitree Z1, xArm",Supervised,,"PointCloud, Proprioception, Vision",,,Diffusion,低コスト・モジュラー型視覚・エクソスケルトンACEは、ハンドポーズ+FK精度を組み合わせ、多種のロボットを高精度で遠隔操作・デモ収集が可能です。シミュレーション・実験でGELLOより2-3倍高速・高精度で、学習データでも80-90 %の組み込みポリシー成功率を達成しました。
VLM for Robotics,Surgical-LVLM,Surgical-LVLM: Learning to Adapt Large Vision-Language Model for Grounded Visual Question Answering in Robotic Surgery,,,https://arxiv.org/abs/2405.10948,,Surgery,Surgical,,"LoRA, Supervised",EndoVis,"Language, Vision",EndoVis,Qwen-VL,,
VLM for Robotics,PaLM-E,PaLM-E: An Embodied Multimodal Language Model,ICML,"Google Research, Robotics at Google, TU Berlin",https://arxiv.org/abs/2303.03378,https://palm-e.github.io/,Manipulation,"Kitchen, Tabletop","Google EDR, xArm",Supervised,Language Table,"Language, Vision",Language Table,PaLM,,PalLM-Eは、画像・センサー信号をLLMエンベデッドに直接注入することで、ロボット計画・VQA・言語課題を一つの巨大モデルで解決し、データ効率・伝達学習・スケーリング効果を実証しました。562Bバージョンは、ロボットインテリジェンスと視覚・言語SOTAを同時に実現した最初のEmbodied巨大マルチモーダルモデルです。
"End-to-End, Policy",Diffusion Policy,Diffusion Policy: Visuomotor Policy Learning via Action Diffusion,RSS,"Columbia University, MIT, Toyota Research Institute",https://arxiv.org/abs/2303.04137v4,https://diffusion-policy.cs.columbia.edu/,Manipulation,Tabletop,"Franka Emika Panda, Simulation, UR5",Supervised,"FrankaKitchen, Push-T, Robomimic","Proprioception, Vision",,,Diffusion,拡散モデルを用いたMulti-Modality対応Policy
VLM for Robotics,VC-1,Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?,NeurIPS,"Georgia Tech, Meta, Stanford University, UC Berkeley, University of Pennsylvania",https://arxiv.org/abs/2303.18240,https://eai-vc.github.io/,"Dexterous Hand, Manipulation, Navigation, Whole-Body Control","Indoor, Tabletop","Franka Emika Panda, Simulation, TriFinger",Self-Supervised,CORTEXBENCH,Vision,"EPIC-Kitchens, Ego4D, ImageNet, Something-Something",ViT,,17課題で構成されたCORTEXBENCHで普遍的なPVR(Pretrained Visual Representations)を見つけたが、VC-1でさえ平均的な最強であり、万能ではない。データの多様性と適応戦略が性能を左右し、将来の「本物の」人工視覚皮質には両方の軸が必要です。
VLM for Robotics,V-JEPA2,"V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning",,Meta,https://arxiv.org/abs/2506.09985,https://ai.meta.com/vjepa/,Manipulation,Tabletop,Franka Emika Panda,Self-Supervised,,Vision,"HowTo100M, ImageNet, Kinetics-700, Something-Something",ViT,,"ウェブスケールビデオ自己教師付きで学習したV-JEPA 2は、モーション理解-QAで新しいSOTAを構築し、62時間のロボット映像だけで得たV-JEPA 2-ACがフランカアームをゼロショットピックアンドプレースまで行う。
つまり、「見て学ぶ」表現ベースの世界モデル1つで理解・予測・計画のすべてを達成した総合ロボットビジョン・世界モデル研究。"
"3D, Affordance",Splat-MOVER,"Splat-MOVER: Multi-Stage, Open-Vocabulary Robotic Manipulation via Editable Gaussian Splatting",CoRL,Stanford University,https://arxiv.org/abs/2405.04378,https://splatmover.github.io/,"Grasp, Manipulation",Tabletop,Kinova Gen3,Self-Supervised,,"Language, PointCloud, Vision",,"CLIP, GraspNet, VRB","Keyframe, Primitive",Gaussian Splattingで3D semantics (3D segmentation + affordance + grasp) を埋め込む
"Affordance, Hierarchical",RT-Affordance,RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation,,"Google DeepMind, University of Texas at Austin",https://arxiv.org/abs/2411.02704,https://snasiriany.me/rt-affordance,Manipulation,Tabletop,Google EDR,Supervised,,"Language, Proprioception, Vision",RT-1,PaLM-E 2,,VLA学習時に物体操作時のアフォーダンス(経路、キーポイント)なども含め、操作学習の中間表現として使うことで性能向上
Affordance,VRB,Affordances from Human Videos as a Versatile Representation for Robotics,CVPR,"Carnegie Mellon University, Meta",https://arxiv.org/abs/2304.08488,https://robo-affordances.github.io/,Manipulation,"Indoor, Kitchen, Tabletop","Franka Emika Panda, Hello Stretch",Self-Supervised,,Vision,"EPIC-Kitchens, Ego4D",,Heatmap,RoboticsのためのVisual Affordance (+2D Trajectory)を人間の動画データから学習
Affordance,General Flow,General Flow as Foundation Affordance for Scalable Robot Learning,CoRL,"Shanghai Artificial Intelligence Laboratory, Tsinghua University",https://arxiv.org/abs/2401.11439,https://general-flow.github.io/,Manipulation,Tabletop,Franka Emika Panda,Supervised,,"Language, PointCloud, Vision",HOI4D,"CLIP, PointNext",Conditional VAE,人間のデモンストレーションなどから物体の3次元動き(Flow)を抽出、学習し、物体操作に応用
"Device, Open-Source, Policy",ALOHA (ACT),Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware,RSS,"Meta, Stanford University, UC Berkeley",https://arxiv.org/abs/2304.13705,https://tonyzhaozh.github.io/aloha/,"Bi-manual, Manipulation",Tabletop,ALOHA,Supervised,,"Proprioception, Vision",,,Conditional VAE,Action Chunking TransformerとLow-Cost Hardwareによる模倣学習
VLM for Robotics,LIV,LIV: Language-Image Representations and Rewards for Robotic Control,ICML,"Meta, University of Pennsylvania",https://arxiv.org/abs/2306.00958,https://penn-pal-lab.github.io/LIV/,Manipulation,"Kitchen, Tabletop",Franka Emika Panda,Self-Supervised,,"Language, Vision",EPIC-Kitchens,CLIP,Continuous,LIVは、VIP(価値学習)とCLIP(視覚・言語ソート)を単一の目的関数に統合し、映像・言語の報酬と表現を同時に学習するロボット指向のVLMです。これにより、事前学習のみで、報酬推定・政策学習・計画において、従来の方法をすべて凌駕し、少量のロボットデータで簡単に性能を引き上げる。
"Dataset, Evaluation",Libero,LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning,NeurIPS,"Sony AI, Tsinghua University, University of Texas at Austin",https://arxiv.org/abs/2306.03310,https://libero-project.github.io/main.html,Manipulation,Tabletop,Simulation,,LIBERO,"Language, Proprioception, Vision",LIBERO,,,LIBEROは、宣言的・手続き的知識転移を同時に扱うロボットLife-Long学習ベンチマークで、130個の操作課題とデモ・分析フレームを提供する。実験の結果、従来のLLアルゴリズムよりも逐次的なファインチューニングがFWTで優れており、アーキテクチャ・課題順序・事前学習が転移に大きな影響を与えることがわかった。
End-to-End,Robocat,RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation,TMLR,Google DeepMind,https://arxiv.org/abs/2306.11706,https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/,Manipulation,Tabletop,"Franka Emika Panda, Kuka iiwa, Sawyer, Simulation",Supervised,,"Proprioception, Vision",,VQ-GAN,"Autoregressive, Discrete Token",視覚目標条件付きディシジョントランスフォーマー(RoboCat)が多ロボット・多課題データを共同学習し、100-1000デモファインチューニング+自己・データ生成ループにより、新しい課題・新しい機構まで素早く適応・性能向上する。
Dataset,RH20T,RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot,RSS,Shanghai Jiao Tong University,https://arxiv.org/abs/2307.00595,https://rh20t.github.io/,Manipulation,Tabletop,"Flexiv Rizon, Franka Emika Panda, Kuka iiwa, UR5",,,"Audio, Depth, Force, Language, Proprioception, Vision",RH20T,,,多様なタスク(contact-richな)とセンサデータ(トルク、オーディオ、RGB、Depthなど)、対応する人間試演を備えた大規模ロボット操作データセット
"3D, Affordance",VoxPoser,VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models,CoRL,"Stanford University, University of Illinois Urbana-Champaign",https://arxiv.org/abs/2307.05973,https://voxposer.github.io/,Manipulation,Tabletop,Franka Emika Panda,,,"Depth, Language, PointCloud, Vision",,"GPT, OwL-ViT, SAM",Optimization,VoxPoserは、LLMがVLMと連携して3Dバリューマップを即座に生成し、モデルベースのMPCでロボット軌跡をゼロショット合成し、さまざまな操作を行います。プリミティブを必要とせず、高い成功率と迅速なオンライン適応を実現し、現実と仮想の両方で高い成功率を達成し、perception依存・プロンプト設計が主な課題となります。
Affordance,VidBot,VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation,CVPR,"ETH Zurich, Microsoft, TUM",https://arxiv.org/abs/2503.07135,https://hanzhic.github.io/vidbot-project/,Manipulation,"Indoor, Kitchen, Tabletop","Hello Stretch, Spot",Self-Supervised,"FrankaKitchen, Maniskill2, PartManip","Depth, Language, PointCloud, Vision",EPIC-Kitchens,"CLIP, DINOv2","Diffusion, Heatmap",人間の動画(Epic-Kitchens)から2次元のVisual Affordanceと3次元の物体操作軌道を抽出、学習してロボットの物体操作に応用
,ViSA-Flow,ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow,,"KTH Royal Institute of Technology, University of Michigan",https://arxiv.org/abs/2505.01288,https://visaflow-web.github.io/ViSAFLOW/,Manipulation,Tabletop,"Franka Emika Panda, Simulation",Self-Supervised,CALVIN,"Language, Proprioception, Vision",Something-Something,"CoTracker, SAM, ViT",Autoregressive,ViSA-Flowは、人間映像から抽出した手・物体Semantic Action Flowでロボットポリシーを事前学習・転移し、10%ロボットデモでもSOTA性能を達成する。重要なのは、セグメンテーション・トラッキング・増幅で作られた表現が、ドメイン間の相互作用構造を整列させる点である。
"Affordance, VLM for Robotics",HRP,HRP: Human Affordances for Robotic Pre-Training,RSS,Carnegie Mellon University,https://arxiv.org/abs/2407.18911,https://hrp-robot.github.io/,Manipulation,"Kitchen, Tabletop","Franka Emika Panda, xArm",Self-Supervised,,Vision,Ego4D,HRP,,"人間の動画からのAffordance (Bounding Box, Hand Param...)などを予測するようにViTを学習、pretrainedされたvision backboneをdownstream taskに適用"
Cross-embodiment,UniSkill,UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representation,,Yonsei University,https://arxiv.org/abs/2505.08787,https://kimhanjung.github.io/UniSkill/,Manipulation,"Kitchen, Tabletop","Franka Emika Panda, WidowX",Self-Supervised,LIBERO,"Depth, Vision","BridgeV2, DROID, H2O, Something-Something",DepthAnything2,Diffusion,大規模な動画データセットからCross-Embodiment的なSkill Representationを学習
"Affordance, Cross-embodiment, Prediction",ATM,Any-point Trajectory Modeling for Policy Learning,RSS,"Shanghai Artificial Intelligence Laboratory, Stanford University, Tsinghua University, UC Berkeley",https://arxiv.org/abs/2401.00025,https://xingyu-lin.github.io/atm/,Manipulation,Tabletop,"Simulation, UR5",Self-Supervised,LIBERO,"Language, Vision",LIBERO,"BERT, ViT",Continuous,Point Trajectory Predictionによるロボットのaction生成によりCross embodiment実現
"Cross-embodiment, Special",Extreme Cross-Embodiment Learning for Manipulation and Navigation,Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation,RSS,"Google DeepMind, Stanford University, UC Berkeley",https://arxiv.org/abs/2402.19432,https://extreme-cross-embodiment.github.io/,"Manipulation, Navigation","Indoor, Outdoor, Tabletop","ALOHA, Jackal, LoCoBot, TELLO, Unitree Go1, WidowX",Supervised,,Vision,"BDD100K, GNM, Open-X-Embodiment",,Diffusion,Manipulation + NavigationにおけるCross-Embodiment Policyの学習
"Cross-embodiment, End-to-End",CrossFormer,"Scaling Cross-Embodied Learning: One Policy for Manipulation, Navigation, Locomotion and Aviation",CoRL,"Carnegie Mellon University, UC Berkeley",https://arxiv.org/abs/2408.11812,https://crossformer-model.github.io/,"Bi-manual, Manipulation, Navigation","Industrial, Kitchen, Outdoor, Tabletop","ALOHA, Franka Emika Panda, LoCoBot, TELLO, Unitree Go1, WidowX",Supervised,,"Language, Proprioception, Vision","BridgeV2, DROID, GNM, Open-X-Embodiment",,"Autoregressive, Continuous",それぞれ別のheadを用意してVLAを学習する
RL,SERL,SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning,ICRA,"Stanford University, UC Berkeley, University of Washington",https://arxiv.org/abs/2401.16013,https://serl-robot.github.io/,"Manipulation, Peg-in-Hole","Assembly, Industrial, Tabletop",Franka Emika Panda,RL,,"Proprioception, Vision",,,Continuous,実世界でのRLのためのフレームワーク
RL,HIL-SERL,Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning,,UC Berkeley,https://arxiv.org/abs/2410.21845,https://hil-serl.github.io/,"Bi-manual, Manipulation, Peg-in-Hole","Assembly, Industrial, Tabletop",Franka Emika Panda,RL,,"Proprioception, Vision",,,Continuous,SERLの学習の加速化のために人間のInterventionを途中で入れる (Human In the Loop)
RL,RLDG,RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning,,UC Berkeley,https://arxiv.org/abs/2412.09858,https://generalist-distillation.github.io/,"Manipulation, Peg-in-Hole","Assembly, Industrial, Tabletop",Franka Emika Panda,"Distillation, RL, Supervised",,"Language, Proprioception, Vision",,"Octo, OpenVLA",,SERLなどのフレームワークで訓練されたTask Specific Agent (Small Agent)でデータを収集してそのデータから大規模VLAを逆DistillationすることでRLによるSkillfullな動作+VLAのGeneral Knowledgeを備えたVLAを訓練
RL,RL-VLM-F,RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback,ICML,"Carnegie Mellon University, University of Southern California",https://arxiv.org/abs/2402.03681,https://rlvlmf2024.github.io/,"Manipulation, Whole-Body Control",Tabletop,Simulation,RL,MetaWorld,"Language, Proprioception, Vision",,"GPT, Gemini",Continuous,VLMが画像ペアの好みを自動ラベル付けして報酬を学習するRL-VLM-Fは、テキスト目標だけで7つのロボット・制御課題を従来の方法よりも安定的に解決しました。つまり、複雑な報酬設計なしで「文章+画像」だけで高性能なポリシーを学習する道を提示します。
RL,VLM-RL,Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning,ICLR,"Anthropic, ETH Zurich",https://arxiv.org/abs/2310.12921,https://sites.google.com/view/vlm-rm,Whole-Body Control,,Simulation,RL,,"Language, Vision",,OpenCLIP,Continuous,事前学習VLM(CLIP)を「ゼロショット報酬モデル」として使用し、自然語1行だけでRL報酬を定義し、モデルが大きいほど複雑な課題を正常に学習した →「大きなVLM = 強力な報酬モデル」というスケーリングの法則が確認された。
End-to-End,RT-2,RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control,CoRL,Google DeepMind,https://arxiv.org/abs/2307.15818,https://robotics-transformer2.github.io/,"Manipulation, Move Base","Indoor, Kitchen, Tabletop",Google EDR,Supervised,,"Language, Vision",RT-1,"PaLI-X, PaLM-E 2","Autoregressive, Discrete Token",first PaLM-E / VLM-based VLA
Dataset,Bridge Data V2,BridgeData V2: A Dataset for Robot Learning at Scale,CoRL,"Carnegie Mellon University, Google DeepMind, Stanford University, UC Berkeley",https://arxiv.org/abs/2308.12952,https://rail-berkeley.github.io/bridgedata/,Manipulation,"Kitchen, Tabletop",WidowX,,,"Language, Proprioception, Vision",BridgeV2,,,Scalableなロボット操作学習のためのWidowXロボットを用いた多様な環境、タスク、物体に対するデータセット
"3D, Affordance",LERF-TOGO,Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping,CoRL,UC Berkeley,https://arxiv.org/abs/2309.07970,https://lerftogo.github.io/desktop.html,Grasp,Tabletop,UR5,Self-Supervised,,"Depth, Language, PointCloud, Vision",,"CLIP, DINO, LERF, ZoeDepth",Keyframe,LERFを拡張し、ロボットのGraspのためのAffordance検出と実際のGrasp
"Hierarchical, Planning, Prediction",HiP,Compositional Foundation Models for Hierarchical Planning,NeurIPS,MIT,https://arxiv.org/abs/2309.08587,https://hierarchical-planning-foundation-model.github.io/,Manipulation,Tabletop,Simulation,Supervised,,"Language, Vision",Ego4D,VC1,,"複数の事前学習済み専門家モデル（言語、ビデオ、行動）を組み合わせて階層的に計画を策定し、各段階ごとの反復的なフィードバックで一貫性を確保するHiPフレームワークを提案。高コストのペアリングデータなしでも、長期複合ロボットタスクにおいて優れた一般化と性能を達成する
."
"Planning, Special",Prompt2Walk,Prompt a Robot to Walk with Large Language Models,CDC,"Tsinghua University, UC Berkeley",https://arxiv.org/abs/2309.09969,https://prompt2walk.github.io/,Whole-Body Control,Outdoor,Simulation,,,"Language, Proprioception",,GPT,,"大型言語モデル（GPT-4）に精密に設計されたテキストプロンプト（説明 + 観察/行動履歴）を付与すると、ファインチューニングなしで4足ロボットの低レベル制御（関節コマンド）を直接生成できることをシミュレーションで実証。
核心手法：RLポリシーから収集した観察/行動ペアをテキストプロンプトの例として使用し、LLMがコンテキスト内での動的フィードバックポリシーを学習し、低レベルコマンドをオートレグレッシブに生成する。"
Device,GELLO,"GELLO: A General, Low-Cost, and Intuitive Teleoperation Framework for Robot Manipulators",IROS,UC Berkeley,https://arxiv.org/abs/2309.13037,https://wuphilipp.github.io/gello_site/,"Bi-manual, Manipulation",Tabletop,"Franka Emika Panda, UR5, xArm",,,Proprioception,,,,"安価な3Dプリント・サーボベースの「縮小ロボットアーム」コントローラーGELLOは、VR-3Dマウスよりも高い成功率と効率でデモの収集品質を向上させる。
オープンソースとして公開され、ロボット模倣学習データ確保の参入障壁を大幅に下げたことが重要な貢献だ。"
End-to-End,NoMAD,NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration,ICRA,UC Berkeley,https://arxiv.org/abs/2310.07896,https://general-navigation-models.github.io/nomad/,Navigation,"Indoor, Outdoor",LoCoBot,Supervised,,Vision,"GNM, SACSoN",,Diffusion,"NoMaDは、ゴールマスクを活用して目標に基づく行動と目標のない探索を単一のディフュージョンポリシーで統合学習し、アクションシーケンスを直接ディフュージョンで生成することで、マルチモーダル行動分布を効果的にモデル化します。
この統一ポリシーは、実際の環境において既存のSOTA（サブゴールディフュージョンなど）と比較して25％以上優れた性能と15倍少ない計算量で、探索とナビゲーションを安定的に実行します。"
"RL, VLM for Robotics",RoboCLIP,RoboCLIP: One Demonstration is Enough to Learn Robot Policies,NeurIPS,"Google Research, Stanford University, UC Berkeley, University of Southern California",https://arxiv.org/abs/2310.07899,https://sites.google.com/view/roboclip/home,Manipulation,"Kitchen, Tabletop",Simulation,RL,"FrankaKitchen, MetaWorld","Language, Vision",HowTo100M,S3D,Continuous,RoboCLIPは、事前学習されたビデオ・言語モデルの埋め込みを活用し、たった1つのビデオまたはテキストデモだけで報酬を生成し、ロボットポリシーをオンラインRLで学習する手法を提案します。別途報酬関数を設計することなく、様々なソースのデモ(ロボット/人間/アニメーション/テキスト)だけで、SOTA ILに比べて優れたゼロショット性能を達成する。
Prediction,AVDC,Learning to Act from Actionless Videos through Dense Correspondences,ICLR,MIT,https://arxiv.org/abs/2310.08576,https://flow-diffusion.github.io/,"Manipulation, Navigation","Indoor, Tabletop","Franka Emika Panda, Simulation",Self-Supervised,"AI2THOR, MetaWorld","Depth, Language, Vision",BridgeV2,"CLIP, GMFlow",Image Generation,"初期フレームからの動画生成で, そのオプティカルフローから頑張って手先のSE3を計算してロボットに送る"
"Cross-embodiment, Dataset, Special",RT-X,Open X-Embodiment: Robotic Learning Datasets and RT-X Models,ICRA,"Carnegie Mellon University, Columbia University, DLR, ETH Zurich, Google DeepMind, Google Research, IIT, Imperial College London, KAIST, Max Planck Institute, Stanford University, The University of Tokyo, Toyota Research Institute, UC Berkeley, UC San Diego, University of Illinois Urbana-Champaign, University of Texas at Austin",https://arxiv.org/abs/2310.08864,https://robotics-transformer-x.github.io/,"Bi-manual, Manipulation","Assembly, Indoor, Industrial, Kitchen, Tabletop","ALOHA, Cobotta, FANUC Mate, Franka Emika Panda, Google EDR, Hello Stretch, Jackal, Kinova Gen3, Kinova jaco, Kuka iiwa, PR2, Sawyer, UR5, Unitree A1, WidowX, xArm",Supervised,,"Language, Proprioception, Vision",Open-X-Embodiment,"RT-1, RT-2","Autoregressive, Discrete Token",多様なEmbodimentを含む大規模なデータセットの作成とそれによるVLA (RT-X)
Prediction,SuSIE,"Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models ",ICLR,"Google DeepMind, Stanford University, UC Berkeley",https://arxiv.org/abs/2310.10639,https://rail-berkeley.github.io/susie/,Manipulation,Tabletop,"Simulation, WidowX",Supervised,CALVIN,"Language, Vision","BridgeV2, Something-Something",,Diffusion,"difffusion modelでsubgoalをpredictionして, そこからの追従はdiffusion modelを別で教師あり学習"
RL,Eureka,Eureka: Human-Level Reward Design via Coding Large Language Models,ICLR,"Caltech, NVIDIA, University of Pennsylvania, University of Texas at Austin",https://arxiv.org/abs/2310.12931,https://eureka-research.github.io/,"Bi-manual, Dexterous Hand, Manipulation, Whole-Body Control","Game, Tabletop",Simulation,RL,,Language,,GPT,Continuous,LLMが報酬コードを直接作成・改善するように進化検索とフィードバックループを接続したEUREKAは、29のRL課題のうち83％で人間の報酬より優れていた。これにより、ハンドペンスピニングのような高度な技術まで自動学習が可能になった。
Dataset,MimicGen,MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations,CoRL,"NVIDIA, University of Texas at Austin",https://arxiv.org/abs/2310.17596,https://mimicgen.github.io/,Manipulation,"Assembly, Kitchen, Tabletop",Simulation,,,"Proprioception, Vision",,,,"MimicGenは、少数の人間のデモンストレーションをオブジェクトベースの軌跡変換によって大規模・多様なデータセットに拡張し、ロボットの学習性能と汎用性を大幅に向上させました。
つまり、「少ない人→多くの合成デモ」パイプラインでデータ収集のボトルネックを革新的に緩和します。"
VLM for Robotics,RoboVQA,RoboVQA: Multimodal Long-Horizon Reasoning for Robotics,ICRA,Google DeepMind,https://arxiv.org/abs/2311.00899,https://robovqa.github.io/,Manipulation,Tabletop,Google EDR,Supervised,,"Language, Vision",RoboVQA,VideoCoCa,"Autoregressive, Discrete Token","様々なオフィス空間でロボットと人間のデモンストレーションをbottom-up方式で大量収集したRoboVQAデータとビデオベースのVLM RoboVQA-VideoCoCaを提案し、実際のロボットの長期推論・計画エラーを従来のSOTAに比べて大幅に削減した。
人間データ活用・タスク増強・ビデオ条件がロボット知能のスケールアップの鍵であることを実験的に実証する。"
End-to-End,RoboFlamingo,Vision-Language Foundation Models as Effective Robot Imitators,,"ByteDance Research, National University of Singapore, Shanghai Jiao Tong University, Tsinghua University",https://arxiv.org/abs/2311.01378,https://roboflamingo.github.io/,Manipulation,Tabletop,Simulation,Supervised,CALVIN,"Language, Proprioception, Vision",CALVIN,OpenFlamingo,Autoregressive,openflamingoで学習されたVLMをそのまま凍結して別のLSTM or Transformerのpolicy headを学習させればよい. RT-1/RT-2みたいにco-finetuningはいらないという主張
"Application-Oriented, End-to-End",RT-Trajectory,RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches,ICLR,"Google DeepMind, Stanford University, UC San Diego",https://arxiv.org/abs/2311.01977,https://rt-trajectory.github.io/,Manipulation,"Kitchen, Tabletop",Google EDR,Supervised,,Vision,RT-1,RT-1,Discrete Token,ロボット作業の一般化を目的として、カメラ画像上に2次元で描かれた概略的な経路（トラジェクトリー・スケッチ）をタスクとして指定し、これを模倣学習で学習するRT-Trajectory方式を提案する。従来の言語/ゴール・コンディショニングに比べて、新しい動作や作業に対する一般化性能が大幅に向上する。
"3D, End-to-End",LEO,An Embodied Generalist Agent in 3D World,ICML,"Carnegie Mellon University, Peking University, Tsinghua University",https://arxiv.org/abs/2311.12871,https://embodied-generalist.github.io/,"Manipulation, Navigation","Indoor, Tabletop",Simulation,Supervised,,"Language, PointCloud, Vision",,"OpenCLIP, PointNet++, Vicuna","Autoregressive, Discrete Token","LEOは、2D/3D視覚入力と言語コマンドを基盤に、知覚、推論、計画、操作を統合的に実行する汎用的なエンボディッドエージェントです。LoRAベースのLLMにマルチモーダルトークン化を通じてインストラクションチューニングで学習されます。
3DシーングラフベースのLLM補助データ生成手法と2段階学習戦略により、既存のタスク特化型モデルよりも優れた性能を多様な3Dタスクで実証しています。"
RL,RPD,Refined Policy Distillation: From VLA Generalists to RL Experts,,University of Technology Nuremberg,https://arxiv.org/abs/2503.05833,,Manipulation,Tabletop,Simulation,"Distillation, RL",Maniskill2,"Language, Vision",Open-X-Embodiment,"Octo, OpenVLA",Continuous,VLA教師の行動をMSE模倣項として組み合わせたPPO学生(RPD)は、サンプル効率・成功率を大幅に高め、軽量・高性能なエキスパートポリシーを自動蒸留する。VLAが少しでも役に立つと、sparse補償・カメラ変更などの困難な条件下でもPPOに比べて優れた性能を発揮する。
"Affordance, RL",KAGI,Affordance-Guided Reinforcement Learning via Visual Prompting,IROS,"Cornell University, Stanford University, UC Berkeley",https://arxiv.org/abs/2407.10341,https://sites.google.com/view/affordance-guided-rl,Manipulation,Tabletop,WidowX,RL,,"Language, Vision",,"GPT, MiniGPT",Continuous,"VLMによるAffordance (Keypointなど)を予測し、それに従うようにRewardを作成, 強化学習を行う"